{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a90797c-501d-460f-a4fd-36e25a78c0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking and downloading required NLTK resources...\n",
      "✓ All NLTK resources ready\n",
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 1\n",
      "DATA LOADING AND PREPROCESSING\n",
      "============================================================\n",
      "✓ Preprocessor initialized\n",
      "✓ Loaded 198 stopwords\n",
      "\n",
      "============================================================\n",
      "LOADING DOCUMENT COLLECTION\n",
      "============================================================\n",
      "Trying encoding: utf-8\n",
      "Trying encoding: latin-1\n",
      "✓ Successfully loaded with latin-1 encoding\n",
      "\n",
      "✓ Loaded 2692 documents from CSV\n",
      "✓ Columns available: ['Article', 'Date', 'Heading', 'NewsType']\n",
      "\n",
      "✓ Stored 2692 documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREPROCESSING DOCUMENTS\n",
      "------------------------------------------------------------\n",
      "Processed 100/2692 documents...\n",
      "Processed 200/2692 documents...\n",
      "Processed 300/2692 documents...\n",
      "Processed 400/2692 documents...\n",
      "Processed 500/2692 documents...\n",
      "Processed 600/2692 documents...\n",
      "Processed 700/2692 documents...\n",
      "Processed 800/2692 documents...\n",
      "Processed 900/2692 documents...\n",
      "Processed 1000/2692 documents...\n",
      "Processed 1100/2692 documents...\n",
      "Processed 1200/2692 documents...\n",
      "Processed 1300/2692 documents...\n",
      "Processed 1400/2692 documents...\n",
      "Processed 1500/2692 documents...\n",
      "Processed 1600/2692 documents...\n",
      "Processed 1700/2692 documents...\n",
      "Processed 1800/2692 documents...\n",
      "Processed 1900/2692 documents...\n",
      "Processed 2000/2692 documents...\n",
      "Processed 2100/2692 documents...\n",
      "Processed 2200/2692 documents...\n",
      "Processed 2300/2692 documents...\n",
      "Processed 2400/2692 documents...\n",
      "Processed 2500/2692 documents...\n",
      "Processed 2600/2692 documents...\n",
      "\n",
      "✓ All 2692 documents preprocessed!\n",
      "\n",
      "============================================================\n",
      "COLLECTION STATISTICS\n",
      "============================================================\n",
      "Total Documents: 2692\n",
      "Average Document Length: 186.33 tokens\n",
      "Vocabulary Size: 18196 unique terms\n",
      "\n",
      "------------------------------------------------------------\n",
      "PREPROCESSING EXAMPLE\n",
      "------------------------------------------------------------\n",
      "Original (first 200 chars):\n",
      "sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "After preprocessing (first 20 tokens):\n",
      "['sindh', 'govt', 'decid', 'cut', 'public', 'transport', 'fare', 'pc', 'kti', 'rej', 'karachi', 'sindh', 'govern', 'decid', 'bring', 'public', 'transport', 'fare', 'per', 'cent']\n",
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "You now have:\n",
      "1. Loaded all news articles\n",
      "2. Cleaned and preprocessed the text\n",
      "3. Created tokens (individual words)\n",
      "4. Removed stopwords and stemmed words\n",
      "\n",
      "Next Phase: We'll implement the Boolean Retrieval Model\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 1: DATA PREPROCESSING\n",
    "============================================================\n",
    "This is the foundation of our IR system. We'll load and preprocess the news articles.\n",
    "\n",
    "Author: Saad Ali\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "print(\"Checking and downloading required NLTK resources...\")\n",
    "\n",
    "# Download punkt tokenizer\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Download punkt_tab (newer version)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    print(\"Downloading punkt_tab...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    \n",
    "# Download stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "print(\"✓ All NLTK resources ready\")\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"\n",
    "    This class handles all text preprocessing tasks.\n",
    "    \n",
    "    Why do we need preprocessing?\n",
    "    - Raw text contains noise (punctuation, special characters)\n",
    "    - Common words like \"the\", \"is\" don't help in retrieval\n",
    "    - Different forms of words (running, runs, ran) should be treated similarly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize stopwords (common words to remove)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize stemmer (reduces words to their root form)\n",
    "        # Example: \"running\" -> \"run\", \"better\" -> \"better\"\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        print(\"✓ Preprocessor initialized\")\n",
    "        print(f\"✓ Loaded {len(self.stop_words)} stopwords\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Step 1: Clean the raw text\n",
    "        - Convert to lowercase\n",
    "        - Remove special characters\n",
    "        - Remove extra whitespace\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Step 2: Split text into individual words (tokens)\n",
    "        Example: \"I love IR\" -> [\"I\", \"love\", \"IR\"]\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"\n",
    "        Step 3: Remove common words that don't add meaning\n",
    "        Example: [\"the\", \"quick\", \"brown\", \"fox\"] -> [\"quick\", \"brown\", \"fox\"]\n",
    "        \"\"\"\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Step 4: Reduce words to their root form\n",
    "        Example: [\"running\", \"runs\", \"ran\"] -> [\"run\", \"run\", \"ran\"]\n",
    "        \"\"\"\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "        Combines all steps: clean -> tokenize -> remove stopwords -> stem\n",
    "        \"\"\"\n",
    "        # Step 1: Clean\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Tokenize\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        \n",
    "        # Step 3: Remove stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Step 4: Stem\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        # Filter out very short tokens (less than 2 characters)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"\n",
    "    This class manages the entire document collection.\n",
    "    It loads, preprocesses, and stores all documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize and load the document collection\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to your CSV file containing news articles\n",
    "        \"\"\"\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []  # Stores original documents\n",
    "        self.processed_docs = []  # Stores preprocessed documents\n",
    "        self.doc_ids = []  # Document identifiers\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING DOCUMENT COLLECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        \"\"\"\n",
    "        Load documents from CSV file with encoding handling\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try multiple encodings to handle different file formats\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    print(f\"Trying encoding: {encoding}\")\n",
    "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "                    print(f\"✓ Successfully loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    if encoding == encodings_to_try[-1]:\n",
    "                        raise e\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read file with any supported encoding\")\n",
    "            \n",
    "            print(f\"\\n✓ Loaded {len(df)} documents from CSV\")\n",
    "            print(f\"✓ Columns available: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Build document text from available columns\n",
    "            for idx, row in df.iterrows():\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                # Check for heading/title column\n",
    "                if 'Heading' in df.columns:\n",
    "                    doc_text += str(row['Heading']) + \" \"\n",
    "                elif 'heading' in df.columns:\n",
    "                    doc_text += str(row['heading']) + \" \"\n",
    "                elif 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                \n",
    "                # Check for article/content column\n",
    "                if 'Article' in df.columns:\n",
    "                    doc_text += str(row['Article'])\n",
    "                elif 'article' in df.columns:\n",
    "                    doc_text += str(row['article'])\n",
    "                elif 'content' in df.columns:\n",
    "                    doc_text += str(row['content'])\n",
    "                elif 'text' in df.columns:\n",
    "                    doc_text += str(row['text'])\n",
    "                \n",
    "                # Store original document\n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                \n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            print(f\"\\n✓ Stored {len(self.documents)} documents\")\n",
    "            \n",
    "            # Preprocess all documents\n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ Error: File not found at {csv_path}\")\n",
    "            print(\"Please make sure you've downloaded the dataset from Kaggle\")\n",
    "            print(\"Check that the file path is correct\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "            print(\"\\nTroubleshooting tips:\")\n",
    "            print(\"1. Check if the file path is correct\")\n",
    "            print(\"2. Make sure the CSV file is not corrupted\")\n",
    "            print(\"3. Try opening the file in a text editor to check its format\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        \"\"\"\n",
    "        Preprocess all documents in the collection\n",
    "        This is done once and stored for efficiency\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"PREPROCESSING DOCUMENTS\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            # Preprocess the document text\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            \n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            # Show progress for every 100 documents\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"\\n✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "        \n",
    "        # Calculate and display statistics\n",
    "        self.display_statistics()\n",
    "    \n",
    "    def display_statistics(self):\n",
    "        \"\"\"\n",
    "        Display useful statistics about the collection\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COLLECTION STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calculate average document length\n",
    "        avg_length = np.mean([doc['token_count'] for doc in self.processed_docs])\n",
    "        \n",
    "        # Calculate vocabulary size (unique terms)\n",
    "        vocabulary = set()\n",
    "        for doc in self.processed_docs:\n",
    "            vocabulary.update(doc['tokens'])\n",
    "        \n",
    "        print(f\"Total Documents: {len(self.documents)}\")\n",
    "        print(f\"Average Document Length: {avg_length:.2f} tokens\")\n",
    "        print(f\"Vocabulary Size: {len(vocabulary)} unique terms\")\n",
    "        \n",
    "        # Show example of preprocessing\n",
    "        if len(self.documents) > 0:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"PREPROCESSING EXAMPLE\")\n",
    "            print(\"-\"*60)\n",
    "            example_text = self.documents[0]['text'][:200]\n",
    "            example_tokens = self.processed_docs[0]['tokens'][:20]\n",
    "            \n",
    "            print(f\"Original (first 200 chars):\\n{example_text}...\")\n",
    "            print(f\"\\nAfter preprocessing (first 20 tokens):\\n{example_tokens}\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        \"\"\"Get original document by ID\"\"\"\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        \"\"\"Get preprocessed document by ID\"\"\"\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    HOW TO USE THIS CODE:\n",
    "    \n",
    "    1. Download the dataset from Kaggle\n",
    "    2. Place the CSV file in the same directory as this script\n",
    "    3. Update the file path below\n",
    "    4. Run this script\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path to your CSV file\n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"  \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 1\")\n",
    "    print(\"DATA LOADING AND PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create the document collection\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou now have:\")\n",
    "    print(\"1. Loaded all news articles\")\n",
    "    print(\"2. Cleaned and preprocessed the text\")\n",
    "    print(\"3. Created tokens (individual words)\")\n",
    "    print(\"4. Removed stopwords and stemmed words\")\n",
    "    print(\"\\nNext Phase: We'll implement the Boolean Retrieval Model\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3971007-c7fd-48e5-96cc-b657ee3a9d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NLTK resources...\n",
      "✓ NLTK resources ready\n",
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 2\n",
      "BOOLEAN RETRIEVAL MODEL\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading documents...\n",
      "✓ Loaded with latin-1 encoding\n",
      "✓ Loaded 2692 documents\n",
      "\n",
      "Preprocessing documents...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "✓ All 2692 documents preprocessed!\n",
      "\n",
      "Step 2: Building inverted index...\n",
      "\n",
      "✓ Inverted Index initialized\n",
      "\n",
      "============================================================\n",
      "BUILDING INVERTED INDEX\n",
      "============================================================\n",
      "  Indexed 500/2692 documents...\n",
      "  Indexed 1000/2692 documents...\n",
      "  Indexed 1500/2692 documents...\n",
      "  Indexed 2000/2692 documents...\n",
      "  Indexed 2500/2692 documents...\n",
      "\n",
      "✓ Inverted index built successfully!\n",
      "✓ Indexed 18196 unique terms\n",
      "\n",
      "------------------------------------------------------------\n",
      "SAMPLE INVERTED INDEX ENTRIES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Term: 'public'\n",
      "  Appears in 172 documents\n",
      "  Sample doc IDs: [0, 1, 2562, 4, 2566]...\n",
      "\n",
      "Term: 'ad'\n",
      "  Appears in 806 documents\n",
      "  Sample doc IDs: [0, 1, 3, 2051, 5]...\n",
      "\n",
      "Term: 'petroleum'\n",
      "  Appears in 202 documents\n",
      "  Sample doc IDs: [0, 1, 3, 4, 7]...\n",
      "\n",
      "Term: 'per'\n",
      "  Appears in 370 documents\n",
      "  Sample doc IDs: [0, 2050, 5, 8, 12]...\n",
      "\n",
      "Term: 'ga'\n",
      "  Appears in 136 documents\n",
      "  Sample doc IDs: [0, 2560, 514, 2569, 523]...\n",
      "\n",
      "Step 3: Initializing Boolean Retrieval Model...\n",
      "\n",
      "✓ Boolean Retrieval Model initialized\n",
      "\n",
      "============================================================\n",
      "QUERY DEMONSTRATIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXECUTING BOOLEAN QUERY: 'karachi'\n",
      "============================================================\n",
      "\n",
      "✓ Found 292 matching documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "Query: 'karachi'\n",
      "Results: 5 documents shown (top 5)\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Document ID: 0\n",
      "   Title: sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   Preview: sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "2. Document ID: 6\n",
      "   Title: bullish kse jumps over 33000 psychological barrier\n",
      "   Preview: bullish kse jumps over 33000 psychological barrier KARACHI: Strong bulls on Friday pulled the benchmark KSE-100 Index at Karachi Stock Exchange (KSE) and taking it across the psychological barrier of ...\n",
      "\n",
      "3. Document ID: 8\n",
      "   Title: sugar prices drop to rs 49.80 in sind\n",
      "   Preview: sugar prices drop to rs 49.80 in sind KARACHI: Wholesale market rates for sugar dropped to less than Rs 50 per kg following the resumption of sugar cane crushing by sugar mills in Sindh.Within two day...\n",
      "\n",
      "4. Document ID: 16\n",
      "   Title: notification issued for reduction in electricity charges for karachi nepr\n",
      "   Preview: notification issued for reduction in electricity charges for karachi nepr ISLAMABAD: The National Electric Power Regulatory Authority (NEPRA) on Wednesday said that a notification has been issued afte...\n",
      "\n",
      "5. Document ID: 1042\n",
      "   Title: Afridi says media blows trivial issues out of proporti\n",
      "   Preview: Afridi says media blows trivial issues out of proporti strong>KARACHI: Shahid Afridi on Wednesday complained that media blows insignificant issues completely out of proportion and then airs them as br...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BOOLEAN QUERY: 'karachi AND transport'\n",
      "============================================================\n",
      "\n",
      "✓ Found 17 matching documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "Query: 'karachi AND transport'\n",
      "Results: 5 documents shown (top 5)\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Document ID: 0\n",
      "   Title: sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   Preview: sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "2. Document ID: 256\n",
      "   Title: adb announces new 1.2 bn annual package for paki\n",
      "   Preview: adb announces new 1.2 bn annual package for paki KARACHI: The Asian Development Bank said Tuesday it has approved a new assistance programme for Pakistan worth $1.2 billion annually, to improve infras...\n",
      "\n",
      "3. Document ID: 2530\n",
      "   Title: Pakistan China commence direct rail and sea freight servi\n",
      "   Preview: Pakistan China commence direct rail and sea freight servi strong>ISLAMABAD/BEIJING: Pakistan and China embarked on their ambitious and effective project to link China's southwestern province Yunnan wi...\n",
      "\n",
      "4. Document ID: 451\n",
      "   Title: PM announces Rs5 per litre cut in POL pri\n",
      "   Preview: PM announces Rs5 per litre cut in POL pri strong>LAHORE: Prime Minister Nawaz Sharif has announced Rs5 per litre cut in petroleum product prices with effect from February 01.</strongThe Premier announ...\n",
      "\n",
      "5. Document ID: 2500\n",
      "   Title: Sindh CM discusses CPEC projects with Chinese envoy\n",
      "   Preview: Sindh CM discusses CPEC projects with Chinese envoy strong>KARACHI: Sindh Chief Minister Murad Ali Shah has said that his government has finalized a plan to establish China-Pakistan Industrial Zone (C...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BOOLEAN QUERY: 'karachi OR lahore'\n",
      "============================================================\n",
      "\n",
      "✓ Found 387 matching documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "Query: 'karachi OR lahore'\n",
      "Results: 5 documents shown (top 5)\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Document ID: 0\n",
      "   Title: sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   Preview: sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "2. Document ID: 6\n",
      "   Title: bullish kse jumps over 33000 psychological barrier\n",
      "   Preview: bullish kse jumps over 33000 psychological barrier KARACHI: Strong bulls on Friday pulled the benchmark KSE-100 Index at Karachi Stock Exchange (KSE) and taking it across the psychological barrier of ...\n",
      "\n",
      "3. Document ID: 1030\n",
      "   Title: Amir hopes to win trust with his perfor\n",
      "   Preview: Amir hopes to win trust with his perfor strong>LAHORE: Pakistans fast bowler Mohamamd Amir, who has been selected for One-day International and Twenty20 for New Zealand series after five-year ban for...\n",
      "\n",
      "4. Document ID: 8\n",
      "   Title: sugar prices drop to rs 49.80 in sind\n",
      "   Preview: sugar prices drop to rs 49.80 in sind KARACHI: Wholesale market rates for sugar dropped to less than Rs 50 per kg following the resumption of sugar cane crushing by sugar mills in Sindh.Within two day...\n",
      "\n",
      "5. Document ID: 2055\n",
      "   Title: Pak women give up first ODI after Knight shines on debut as captai\n",
      "   Preview: Pak women give up first ODI after Knight shines on debut as captai LONDON: Heather Knight had a dream first match as England women´s captain starring with both bat and ball in a seven-wicket win over ...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BOOLEAN QUERY: 'government NOT corruption'\n",
      "============================================================\n",
      "\n",
      "✓ Found 488 matching documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "Query: 'government NOT corruption'\n",
      "Results: 5 documents shown (top 5)\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Document ID: 0\n",
      "   Title: sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   Preview: sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "2. Document ID: 3\n",
      "   Title: asian stocks sink euro near nine year \n",
      "   Preview: asian stocks sink euro near nine year  HONG KONG: Asian markets tumbled Tuesday following painful losses in New York and Europe while the euro sat near nine-year lows as political uncertainty in Greec...\n",
      "\n",
      "3. Document ID: 14\n",
      "   Title: brent crude steady around 50\n",
      "   Preview: brent crude steady around 50 SINGAPORE: Brent crude oil prices traded around $50 a barrel on Monday, with some support coming from falling US output growth but an expectation of weak Chinese economic ...\n",
      "\n",
      "4. Document ID: 18\n",
      "   Title: ecc approves export of 1.2 million tons of \n",
      "   Preview: ecc approves export of 1.2 million tons of  ISLAMABAD: The Economic Coordination Committee of the Cabinet (ECC) on Friday approved export of 1.2 million tons of wheat and imposed a bans on import of w...\n",
      "\n",
      "5. Document ID: 21\n",
      "   Title: finance minister apprised on pak brunei investment \n",
      "   Preview: finance minister apprised on pak brunei investment  ISLAMABAD: Finance Minister Ishaq Dar here on Sunday chaired a briefing session by managing director Pak-Brunei Investment Company seeking details o...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BOOLEAN QUERY: 'pakistan AND election'\n",
      "============================================================\n",
      "\n",
      "✓ Found 48 matching documents\n",
      "\n",
      "------------------------------------------------------------\n",
      "Query: 'pakistan AND election'\n",
      "Results: 5 documents shown (top 5)\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Document ID: 2561\n",
      "   Title: Finance ministry dispels demonetization rumor\n",
      "   Preview: Finance ministry dispels demonetization rumor strong>ISLAMABAD: The finance ministry Monday categorically denied the news reports and rumors circulating in a section of press and some business circles...\n",
      "\n",
      "2. Document ID: 1282\n",
      "   Title: Quetta Gladiators bat against Islamabad United in PSL Fi\n",
      "   Preview: Quetta Gladiators bat against Islamabad United in PSL Fi strong>DUBAI: Islamabad United captain Misbah-ul-Haq won the toss and elected to field first against Quetta Gladiators in the final of the firs...\n",
      "\n",
      "3. Document ID: 1155\n",
      "   Title: Quetta Gladiators Win Toss invite Islamabad United to bat in First PSL \n",
      "   Preview: Quetta Gladiators Win Toss invite Islamabad United to bat in First PSL  DUBAI: Quetta Gladiators won the toss and elected to field, inviting Islamabad United to bat first in the very first T20 match o...\n",
      "\n",
      "4. Document ID: 135\n",
      "   Title: sp raises pakistans outlook forecasts higher gr\n",
      "   Preview: sp raises pakistans outlook forecasts higher gr KARACHI: Standard and Poor's on Tuesday revised Pakistan's credit rating outlook from stable to positive and forecast higher GDP growth for 2015 to 2017...\n",
      "\n",
      "5. Document ID: 1160\n",
      "   Title: United set 146 to beat Zalmi in PSLs third \n",
      "   Preview: United set 146 to beat Zalmi in PSLs third  DUBAI:  Peshawar Zalmi made 145-7 against Islamabad United after their captain Shahid Afridi won the toss and elected to bat in the third match of the Pakis...\n",
      "\n",
      "============================================================\n",
      "PHASE 2 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Key Achievements:\n",
      "✓ Built an inverted index for fast retrieval\n",
      "✓ Implemented Boolean operators (AND, OR, NOT)\n",
      "✓ Demonstrated different query types\n",
      "\n",
      "Limitations of Boolean Model:\n",
      "- No ranking (all results are equally relevant)\n",
      "- Exact matching only (no partial matches)\n",
      "- Can return too many or too few results\n",
      "\n",
      "Next Phase: TF-IDF Model (with ranking!)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 2: BOOLEAN RETRIEVAL MODEL\n",
    "================================================================\n",
    "This implements the Boolean Retrieval Model using an inverted index.\n",
    "\n",
    "What is Boolean Retrieval?\n",
    "- Uses AND, OR, NOT operators to find documents\n",
    "- Example: \"election AND politics\" finds docs with BOTH terms\n",
    "- Fast and precise, but no ranking (results are binary: match or no match)\n",
    "\n",
    "Author: Saad Ali\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Set, Dict\n",
    "\n",
    "# Download required NLTK data (if not already downloaded)\n",
    "print(\"Checking NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✓ NLTK resources ready\")\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1 CLASSES (Embedded for convenience)\n",
    "# ============================================================\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Handles all text preprocessing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"Manages the entire document collection\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.doc_ids = []\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        try:\n",
    "            # Try multiple encodings\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "                    print(f\"✓ Loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read file with any supported encoding\")\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df)} documents\")\n",
    "            \n",
    "            # Build documents from CSV\n",
    "            for idx, row in df.iterrows():\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                # Check for heading/title\n",
    "                if 'Heading' in df.columns:\n",
    "                    doc_text += str(row['Heading']) + \" \"\n",
    "                elif 'heading' in df.columns:\n",
    "                    doc_text += str(row['heading']) + \" \"\n",
    "                elif 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                \n",
    "                # Check for article/content\n",
    "                if 'Article' in df.columns:\n",
    "                    doc_text += str(row['Article'])\n",
    "                elif 'article' in df.columns:\n",
    "                    doc_text += str(row['article'])\n",
    "                elif 'content' in df.columns:\n",
    "                    doc_text += str(row['content'])\n",
    "                elif 'text' in df.columns:\n",
    "                    doc_text += str(row['text'])\n",
    "                \n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            # Preprocess all documents\n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        print(\"\\nPreprocessing documents...\")\n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: BOOLEAN RETRIEVAL MODEL\n",
    "# ============================================================\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"\n",
    "    An Inverted Index is the core data structure for efficient retrieval.\n",
    "    \n",
    "    What is an Inverted Index?\n",
    "    ----------------------------\n",
    "    Instead of storing \"which words are in each document\",\n",
    "    we store \"which documents contain each word\".\n",
    "    \n",
    "    Example:\n",
    "    Doc1: \"cat dog\"\n",
    "    Doc2: \"dog bird\"\n",
    "    Doc3: \"cat bird\"\n",
    "    \n",
    "    Inverted Index:\n",
    "    \"cat\" -> [Doc1, Doc3]\n",
    "    \"dog\" -> [Doc1, Doc2]\n",
    "    \"bird\" -> [Doc2, Doc3]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_doc_freq = defaultdict(int)\n",
    "        self.num_documents = 0\n",
    "        print(\"\\n✓ Inverted Index initialized\")\n",
    "    \n",
    "    def build_index(self, processed_docs: List[Dict]):\n",
    "        \"\"\"Build the inverted index from preprocessed documents\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BUILDING INVERTED INDEX\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.num_documents = len(processed_docs)\n",
    "        \n",
    "        for doc in processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            \n",
    "            # Get unique terms in this document\n",
    "            unique_terms = set(tokens)\n",
    "            \n",
    "            # For each unique term, add this document to its posting list\n",
    "            for term in unique_terms:\n",
    "                self.index[term].add(doc_id)\n",
    "                self.term_doc_freq[term] += 1\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Indexed {doc_id + 1}/{self.num_documents} documents...\")\n",
    "        \n",
    "        print(f\"\\n✓ Inverted index built successfully!\")\n",
    "        print(f\"✓ Indexed {len(self.index)} unique terms\")\n",
    "        \n",
    "        self.display_sample_entries()\n",
    "    \n",
    "    def display_sample_entries(self):\n",
    "        \"\"\"Display sample inverted index entries\"\"\"\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"SAMPLE INVERTED INDEX ENTRIES\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        sample_terms = list(self.index.keys())[:5]\n",
    "        \n",
    "        for term in sample_terms:\n",
    "            doc_ids = list(self.index[term])[:5]\n",
    "            print(f\"\\nTerm: '{term}'\")\n",
    "            print(f\"  Appears in {len(self.index[term])} documents\")\n",
    "            print(f\"  Sample doc IDs: {doc_ids}...\")\n",
    "    \n",
    "    def get_postings(self, term: str) -> Set[int]:\n",
    "        \"\"\"Get the posting list for a term\"\"\"\n",
    "        return self.index.get(term, set())\n",
    "    \n",
    "    def get_term_frequency(self, term: str) -> int:\n",
    "        \"\"\"Get document frequency of a term\"\"\"\n",
    "        return self.term_doc_freq.get(term, 0)\n",
    "\n",
    "\n",
    "class BooleanRetrievalModel:\n",
    "    \"\"\"\n",
    "    Implements Boolean Retrieval with AND, OR, NOT operators\n",
    "    \n",
    "    Boolean Operators:\n",
    "    ------------------\n",
    "    - AND: Documents must contain ALL terms (intersection)\n",
    "    - OR: Documents must contain AT LEAST ONE term (union)\n",
    "    - NOT: Exclude documents containing the term (difference)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inverted_index: InvertedIndex, preprocessor: DocumentPreprocessor):\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = preprocessor\n",
    "        print(\"\\n✓ Boolean Retrieval Model initialized\")\n",
    "    \n",
    "    def parse_query(self, query: str) -> List[str]:\n",
    "        \"\"\"Parse and preprocess the query\"\"\"\n",
    "        tokens = []\n",
    "        parts = query.split()\n",
    "        \n",
    "        for part in parts:\n",
    "            upper_part = part.upper()\n",
    "            if upper_part in ['AND', 'OR', 'NOT']:\n",
    "                tokens.append(upper_part)\n",
    "            else:\n",
    "                processed_terms = self.preprocessor.preprocess(part)\n",
    "                tokens.extend(processed_terms)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def execute_query(self, query: str) -> Set[int]:\n",
    "        \"\"\"Execute a Boolean query and return matching document IDs\"\"\"\n",
    "        tokens = self.parse_query(query)\n",
    "        \n",
    "        if not tokens:\n",
    "            return set()\n",
    "        \n",
    "        if 'AND' in tokens:\n",
    "            return self._execute_and_query(tokens)\n",
    "        elif 'OR' in tokens:\n",
    "            return self._execute_or_query(tokens)\n",
    "        elif 'NOT' in tokens:\n",
    "            return self._execute_not_query(tokens)\n",
    "        else:\n",
    "            return self._execute_or_query(tokens)\n",
    "    \n",
    "    def _execute_and_query(self, tokens: List[str]) -> Set[int]:\n",
    "        \"\"\"Execute AND query (intersection)\"\"\"\n",
    "        term_groups = []\n",
    "        current_group = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token == 'AND':\n",
    "                if current_group:\n",
    "                    term_groups.append(current_group)\n",
    "                    current_group = []\n",
    "            else:\n",
    "                current_group.append(token)\n",
    "        \n",
    "        if current_group:\n",
    "            term_groups.append(current_group)\n",
    "        \n",
    "        result = None\n",
    "        \n",
    "        for group in term_groups:\n",
    "            group_result = set()\n",
    "            for term in group:\n",
    "                group_result = group_result.union(self.index.get_postings(term))\n",
    "            \n",
    "            if result is None:\n",
    "                result = group_result\n",
    "            else:\n",
    "                result = result.intersection(group_result)\n",
    "        \n",
    "        return result if result else set()\n",
    "    \n",
    "    def _execute_or_query(self, tokens: List[str]) -> Set[int]:\n",
    "        \"\"\"Execute OR query (union)\"\"\"\n",
    "        result = set()\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token not in ['OR', 'AND', 'NOT']:\n",
    "                result = result.union(self.index.get_postings(token))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _execute_not_query(self, tokens: List[str]) -> Set[int]:\n",
    "        \"\"\"Execute NOT query (difference)\"\"\"\n",
    "        not_index = tokens.index('NOT')\n",
    "        \n",
    "        include_terms = [t for t in tokens[:not_index] if t not in ['AND', 'OR', 'NOT']]\n",
    "        exclude_terms = [t for t in tokens[not_index+1:] if t not in ['AND', 'OR', 'NOT']]\n",
    "        \n",
    "        result = set()\n",
    "        for term in include_terms:\n",
    "            result = result.union(self.index.get_postings(term))\n",
    "        \n",
    "        if not result:\n",
    "            result = set(range(self.index.num_documents))\n",
    "        \n",
    "        for term in exclude_terms:\n",
    "            result = result.difference(self.index.get_postings(term))\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def search(self, query: str, collection: DocumentCollection, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Perform a search and return results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"EXECUTING BOOLEAN QUERY: '{query}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        matching_doc_ids = self.execute_query(query)\n",
    "        \n",
    "        print(f\"\\n✓ Found {len(matching_doc_ids)} matching documents\")\n",
    "        \n",
    "        results = []\n",
    "        for doc_id in list(matching_doc_ids)[:top_k]:\n",
    "            doc = collection.get_document(doc_id)\n",
    "            \n",
    "            # Get title from original data\n",
    "            title = \"N/A\"\n",
    "            if 'Heading' in doc['original']:\n",
    "                title = doc['original']['Heading']\n",
    "            elif 'heading' in doc['original']:\n",
    "                title = doc['original']['heading']\n",
    "            elif 'title' in doc['original']:\n",
    "                title = doc['original']['title']\n",
    "            \n",
    "            results.append({\n",
    "                'doc_id': doc_id,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:200] + \"...\",\n",
    "                'relevance': 'MATCH'\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USAGE EXAMPLE AND DEMONSTRATION\n",
    "# ============================================================\n",
    "\n",
    "def demonstrate_boolean_retrieval():\n",
    "    \"\"\"Complete demonstration of Boolean Retrieval Model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 2\")\n",
    "    print(\"BOOLEAN RETRIEVAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Load and preprocess documents\n",
    "    print(\"\\nStep 1: Loading documents...\")\n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"  # Update this path\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    # Step 2: Build inverted index\n",
    "    print(\"\\nStep 2: Building inverted index...\")\n",
    "    inverted_index = InvertedIndex()\n",
    "    inverted_index.build_index(collection.processed_docs)\n",
    "    \n",
    "    # Step 3: Initialize Boolean Retrieval Model\n",
    "    print(\"\\nStep 3: Initializing Boolean Retrieval Model...\")\n",
    "    boolean_model = BooleanRetrievalModel(inverted_index, collection.preprocessor)\n",
    "    \n",
    "    # Step 4: Demonstrate different types of queries\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUERY DEMONSTRATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Example queries (adjust based on your dataset)\n",
    "    queries = [\n",
    "        \"karachi\",                           # Single term\n",
    "        \"karachi AND transport\",             # AND query\n",
    "        \"karachi OR lahore\",                 # OR query\n",
    "        \"government NOT corruption\",         # NOT query\n",
    "        \"pakistan AND election\"              # Multiple terms with AND\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        results = boolean_model.search(query, collection, top_k=5)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(f\"Results: {len(results)} documents shown (top 5)\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Document ID: {result['doc_id']}\")\n",
    "            print(f\"   Title: {result['title']}\")\n",
    "            print(f\"   Preview: {result['text_preview']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nKey Achievements:\")\n",
    "    print(\"✓ Built an inverted index for fast retrieval\")\n",
    "    print(\"✓ Implemented Boolean operators (AND, OR, NOT)\")\n",
    "    print(\"✓ Demonstrated different query types\")\n",
    "    print(\"\\nLimitations of Boolean Model:\")\n",
    "    print(\"- No ranking (all results are equally relevant)\")\n",
    "    print(\"- Exact matching only (no partial matches)\")\n",
    "    print(\"- Can return too many or too few results\")\n",
    "    print(\"\\nNext Phase: TF-IDF Model (with ranking!)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_boolean_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c944c4fc-385f-4cd6-9d5a-386a55077735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NLTK resources...\n",
      "✓ NLTK resources ready\n",
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 3\n",
      "TF-IDF RETRIEVAL MODEL\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading documents...\n",
      "✓ Loaded with latin-1 encoding\n",
      "✓ Loaded 2692 documents\n",
      "\n",
      "Preprocessing documents...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "✓ All 2692 documents preprocessed!\n",
      "\n",
      "Step 2: Building inverted index...\n",
      "\n",
      "Building inverted index...\n",
      "  Indexed 500/2692 documents...\n",
      "  Indexed 1000/2692 documents...\n",
      "  Indexed 1500/2692 documents...\n",
      "  Indexed 2000/2692 documents...\n",
      "  Indexed 2500/2692 documents...\n",
      "✓ Indexed 18196 unique terms\n",
      "\n",
      "Step 3: Initializing TF-IDF Model...\n",
      "\n",
      "✓ TF-IDF Model initialized\n",
      "\n",
      "============================================================\n",
      "BUILDING TF-IDF MATRIX\n",
      "============================================================\n",
      "\n",
      "Step 1: Calculating term frequencies...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "\n",
      "Step 2: Calculating IDF values...\n",
      "✓ Calculated IDF for 18196 unique terms\n",
      "\n",
      "Step 3: Calculating TF-IDF scores...\n",
      "  Calculated TF-IDF for 500/2692 documents...\n",
      "  Calculated TF-IDF for 1000/2692 documents...\n",
      "  Calculated TF-IDF for 1500/2692 documents...\n",
      "  Calculated TF-IDF for 2000/2692 documents...\n",
      "  Calculated TF-IDF for 2500/2692 documents...\n",
      "\n",
      "✓ TF-IDF matrix built successfully!\n",
      "\n",
      "============================================================\n",
      "TF-IDF STATISTICS\n",
      "============================================================\n",
      "\n",
      "Top 10 Most Discriminative Terms (Highest IDF):\n",
      "------------------------------------------------------------\n",
      " 1. 'kti' - IDF: 7.2049 (in 1 docs)\n",
      " 2. 'rickshaw' - IDF: 7.2049 (in 1 docs)\n",
      " 3. 'rej' - IDF: 7.2049 (in 1 docs)\n",
      " 4. 'wilmar' - IDF: 7.2049 (in 1 docs)\n",
      " 5. 'megah' - IDF: 7.2049 (in 1 docs)\n",
      " 6. 'vank' - IDF: 7.2049 (in 1 docs)\n",
      " 7. 'csr' - IDF: 7.2049 (in 1 docs)\n",
      " 8. 'cnr' - IDF: 7.2049 (in 1 docs)\n",
      " 9. 'tambangraya' - IDF: 7.2049 (in 1 docs)\n",
      "10. 'moribund' - IDF: 7.2049 (in 1 docs)\n",
      "\n",
      "Top 10 Most Common Terms (Lowest IDF):\n",
      "------------------------------------------------------------\n",
      " 1. 'said' - IDF: 0.4070 (in 1791 docs)\n",
      " 2. 'strong' - IDF: 0.4932 (in 1643 docs)\n",
      " 3. 'year' - IDF: 0.6091 (in 1463 docs)\n",
      " 4. 'first' - IDF: 0.7711 (in 1244 docs)\n",
      " 5. 'also' - IDF: 0.7752 (in 1239 docs)\n",
      " 6. 'last' - IDF: 0.8189 (in 1186 docs)\n",
      " 7. 'two' - IDF: 0.8610 (in 1137 docs)\n",
      " 8. 'one' - IDF: 0.8619 (in 1136 docs)\n",
      " 9. 'world' - IDF: 0.8761 (in 1120 docs)\n",
      "10. 'day' - IDF: 1.0125 (in 977 docs)\n",
      "\n",
      "============================================================\n",
      "QUERY DEMONSTRATIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXECUTING TF-IDF QUERY: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['karachi', 'transport', 'govern']\n",
      "\n",
      "Query TF-IDF scores:\n",
      "  'karachi': 0.7393\n",
      "  'transport': 1.3092\n",
      "  'govern': 0.5513\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 754 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "1. [Score: 0.4935] Doc ID: 124\n",
      "   Title: goods transport by cargo trains goes up 10 ti\n",
      "   Preview: goods transport by cargo trains goes up 10 ti KARACHI: The transportation of cargo through freight trains has witnessed a marked improvement.According to Javed Anwar, the chief administrator of Pakist...\n",
      "\n",
      "2. [Score: 0.4621] Doc ID: 2562\n",
      "   Title: Iran offers to provide buses for public transport sy\n",
      "   Preview: Iran offers to provide buses for public transport sy strong>ISLAMABAD: Iran has offered to provide modern buses to improve public transport in Pakistan, especially in Sindh province.</strongAccording ...\n",
      "\n",
      "3. [Score: 0.4269] Doc ID: 0\n",
      "   Title: sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   Preview: sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 per cent due to massive reduction in petroleum produ...\n",
      "\n",
      "4. [Score: 0.1395] Doc ID: 2530\n",
      "   Title: Pakistan China commence direct rail and sea freight servi\n",
      "   Preview: Pakistan China commence direct rail and sea freight servi strong>ISLAMABAD/BEIJING: Pakistan and China embarked on their ambitious and effective project to link China's southwestern province Yunnan wi...\n",
      "\n",
      "5. [Score: 0.1118] Doc ID: 2663\n",
      "   Title: CPEC China approves huge infrastructure projects for Xinjiang\n",
      "   Preview: CPEC China approves huge infrastructure projects for Xinjiang strong>BEIJING: The Xinjiang Uygur autonomous region will invest heavily in more high-speed railways and highways to build a high-quality ...\n",
      "\n",
      "============================================================\n",
      "EXECUTING TF-IDF QUERY: 'pakistan cricket victory'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['pakistan', 'cricket', 'victori']\n",
      "\n",
      "Query TF-IDF scores:\n",
      "  'pakistan': 0.3490\n",
      "  'cricket': 0.5026\n",
      "  'victori': 0.7078\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 1385 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'pakistan cricket victory'\n",
      "============================================================\n",
      "\n",
      "1. [Score: 0.1136] Doc ID: 1482\n",
      "   Title: Imran congratulates womens team criticizes Pakistan cricket structur\n",
      "   Preview: Imran congratulates womens team criticizes Pakistan cricket structur strong>ISLAMABAD: Former captain Imran Khan on Sunday congratulated Pakistan Women cricket team on their victory against India.</st...\n",
      "\n",
      "2. [Score: 0.0887] Doc ID: 1746\n",
      "   Title: ICC President discusses cricket with Punjab CM\n",
      "   Preview: ICC President discusses cricket with Punjab CM LAHORE: International Cricket Council (ICC) President Zaheer Abbas here Saturday called on Punjab Chief Minister Muhammad Shehbaz Sharif and discussed th...\n",
      "\n",
      "3. [Score: 0.0859] Doc ID: 1251\n",
      "   Title: Indian HC ensures VVIP security for Pakistan cricket \n",
      "   Preview: Indian HC ensures VVIP security for Pakistan cricket  Indian HC ensures VVIP security for Pakistan cricket teamLAHORE: Indian high commissioner Gautam Bambawale assured that the Pakistan cricket team ...\n",
      "\n",
      "4. [Score: 0.0805] Doc ID: 1403\n",
      "   Title: Govt to decide on teams departure PCB\n",
      "   Preview: Govt to decide on teams departure PCB strong>LAHORE: The Federal government has been asked by the Pakistan Cricket Board (PCB) to decide on cricket teams departure to India for taking part in the Wor...\n",
      "\n",
      "5. [Score: 0.0723] Doc ID: 1145\n",
      "   Title: Pakistan Cricket Board Considering Central Contract For Mohammad Amir\n",
      "   Preview: Pakistan Cricket Board Considering Central Contract For Mohammad Amir strong>LAHORE: Mohammad Amir, who came back to international cricket after five years during the series against New Zealand, is on...\n",
      "\n",
      "============================================================\n",
      "EXECUTING TF-IDF QUERY: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['lahor', 'educ', 'univers']\n",
      "\n",
      "Query TF-IDF scores:\n",
      "  'lahor': 0.9879\n",
      "  'educ': 1.2735\n",
      "  'univers': 1.3868\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 217 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "1. [Score: 0.4616] Doc ID: 1104\n",
      "   Title: Green shirts Kiwis observe one minute silence for university martyr\n",
      "   Preview: Green shirts Kiwis observe one minute silence for university martyr strong>WELLINGTON: Pakistan and New Zealand teams observed one-minute silence in the memory of Bacha khan University attack martyrs ...\n",
      "\n",
      "2. [Score: 0.2899] Doc ID: 696\n",
      "   Title: Budget HEC to get Rs 21486487 mln for higher educati\n",
      "   Preview: Budget HEC to get Rs 21486487 mln for higher educati strong>ISLAMABAD: The government has allocated Rs 21486.487 million for the Higher Education Commission (HEC) for the Fiscal Year 2016-17 in the Pu...\n",
      "\n",
      "3. [Score: 0.1877] Doc ID: 1178\n",
      "   Title: PSL Peshawar Zalmi tames Lahore Qalandars with 9 wi\n",
      "   Preview: PSL Peshawar Zalmi tames Lahore Qalandars with 9 wi LAHORE: Peshawar Zalmi secured a second consecutive win by defeating Lahore Qalandars by 9 wickets in the fifth match of Paksitan Super League (PSL)...\n",
      "\n",
      "4. [Score: 0.1785] Doc ID: 1278\n",
      "   Title: National Women Netball Championship to kick off on March 3\n",
      "   Preview: National Women Netball Championship to kick off on March 3 strong>KARACHI: Pakistan Netball Federation (PNF) is set to organize 15th National Women Netball Championship from March 3 at Pakistan Sports...\n",
      "\n",
      "5. [Score: 0.1768] Doc ID: 2437\n",
      "   Title: PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year\n",
      "   Preview: PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year strong>ISLAMABAD: Prime Minister Nawaz Sharif, Wednesday emphasizing the need to fully tap the business potential between Pakistan an...\n",
      "\n",
      "============================================================\n",
      "PHASE 3 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "Key Achievements:\n",
      "✓ Implemented TF-IDF scoring\n",
      "✓ Built TF-IDF matrix for all documents\n",
      "✓ Implemented document ranking\n",
      "\n",
      "Advantages of TF-IDF:\n",
      "✓ Ranked results (most relevant first)\n",
      "✓ Handles term importance (rare terms weighted higher)\n",
      "✓ Better user experience than Boolean model\n",
      "✓ More flexible queries\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 3: TF-IDF RETRIEVAL MODEL\n",
    "===============================================================\n",
    "This implements the TF-IDF (Term Frequency-Inverse Document Frequency) Model.\n",
    "\n",
    "What is TF-IDF?\n",
    "---------------\n",
    "TF-IDF is a numerical statistic that reflects how important a word is to a \n",
    "document in a collection. It combines two concepts:\n",
    "\n",
    "1. TF (Term Frequency): How often does a term appear in THIS document?\n",
    "2. IDF (Inverse Document Frequency): How rare is this term across ALL documents?\n",
    "\n",
    "Key Innovation: TF-IDF provides RANKING - documents are scored and ordered!\n",
    "\n",
    "Author: Saad Ali\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Checking NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✓ NLTK resources ready\")\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1 & 2 CLASSES (Embedded for convenience)\n",
    "# ============================================================\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Handles all text preprocessing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"Manages the entire document collection\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.doc_ids = []\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        try:\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "                    print(f\"✓ Loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read file\")\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df)} documents\")\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                if 'Heading' in df.columns:\n",
    "                    doc_text += str(row['Heading']) + \" \"\n",
    "                elif 'heading' in df.columns:\n",
    "                    doc_text += str(row['heading']) + \" \"\n",
    "                elif 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                \n",
    "                if 'Article' in df.columns:\n",
    "                    doc_text += str(row['Article'])\n",
    "                elif 'article' in df.columns:\n",
    "                    doc_text += str(row['article'])\n",
    "                elif 'content' in df.columns:\n",
    "                    doc_text += str(row['content'])\n",
    "                elif 'text' in df.columns:\n",
    "                    doc_text += str(row['text'])\n",
    "                \n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        print(\"\\nPreprocessing documents...\")\n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"Inverted Index for efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_doc_freq = defaultdict(int)\n",
    "        self.num_documents = 0\n",
    "    \n",
    "    def build_index(self, processed_docs: List[Dict]):\n",
    "        \"\"\"Build the inverted index\"\"\"\n",
    "        print(\"\\nBuilding inverted index...\")\n",
    "        \n",
    "        self.num_documents = len(processed_docs)\n",
    "        \n",
    "        for doc in processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            unique_terms = set(tokens)\n",
    "            \n",
    "            for term in unique_terms:\n",
    "                self.index[term].add(doc_id)\n",
    "                self.term_doc_freq[term] += 1\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Indexed {doc_id + 1}/{self.num_documents} documents...\")\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.index)} unique terms\")\n",
    "    \n",
    "    def get_postings(self, term: str) -> Set[int]:\n",
    "        return self.index.get(term, set())\n",
    "    \n",
    "    def get_term_frequency(self, term: str) -> int:\n",
    "        return self.term_doc_freq.get(term, 0)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 3: TF-IDF MODEL\n",
    "# ============================================================\n",
    "\n",
    "class TFIDFModel:\n",
    "    \"\"\"\n",
    "    Implements TF-IDF Retrieval Model with Ranking\n",
    "    \n",
    "    Mathematical Formulas:\n",
    "    ----------------------\n",
    "    1. Term Frequency (TF):\n",
    "       TF(t, d) = (Number of times term t appears in document d) / \n",
    "                  (Total number of terms in document d)\n",
    "    \n",
    "    2. Inverse Document Frequency (IDF):\n",
    "       IDF(t) = log(Total documents / Documents containing term t)\n",
    "    \n",
    "    3. TF-IDF Score:\n",
    "       TF-IDF(t, d) = TF(t, d) × IDF(t)\n",
    "    \n",
    "    4. Document Score (for a query):\n",
    "       Score(q, d) = Σ TF-IDF(t, d) for all terms t in query q\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection: DocumentCollection, inverted_index: InvertedIndex):\n",
    "        \"\"\"Initialize TF-IDF Model\"\"\"\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        \n",
    "        # TF-IDF matrix: {doc_id: {term: tf_idf_score}}\n",
    "        self.tfidf_matrix = {}\n",
    "        \n",
    "        # IDF values: {term: idf_value}\n",
    "        self.idf_values = {}\n",
    "        \n",
    "        # Term frequencies: {doc_id: {term: count}}\n",
    "        self.term_frequencies = {}\n",
    "        \n",
    "        print(\"\\n✓ TF-IDF Model initialized\")\n",
    "        self.build_tfidf_matrix()\n",
    "    \n",
    "    def calculate_tf(self, term_count: int, total_terms: int, method: str = 'normalized') -> float:\n",
    "        \"\"\"\n",
    "        Calculate Term Frequency (TF)\n",
    "        \n",
    "        Methods:\n",
    "        - 'raw': Just the count\n",
    "        - 'normalized': count / total_terms (most common)\n",
    "        - 'log': 1 + log(count) if count > 0\n",
    "        \"\"\"\n",
    "        if term_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        if method == 'raw':\n",
    "            return float(term_count)\n",
    "        elif method == 'normalized':\n",
    "            return term_count / total_terms if total_terms > 0 else 0.0\n",
    "        elif method == 'log':\n",
    "            return 1.0 + math.log(term_count)\n",
    "        else:\n",
    "            return term_count / total_terms if total_terms > 0 else 0.0\n",
    "    \n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate Inverse Document Frequency (IDF)\n",
    "        \n",
    "        Formula: IDF(t) = log(N / df(t))\n",
    "        Where:\n",
    "        - N = total number of documents\n",
    "        - df(t) = number of documents containing term t\n",
    "        \"\"\"\n",
    "        N = self.index.num_documents\n",
    "        df = self.index.get_term_frequency(term)\n",
    "        \n",
    "        if df == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Add 1 to denominator for smoothing\n",
    "        idf = math.log(N / (1 + df))\n",
    "        \n",
    "        return idf\n",
    "    \n",
    "    def build_tfidf_matrix(self):\n",
    "        \"\"\"Build TF-IDF matrix for all documents\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BUILDING TF-IDF MATRIX\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Calculate term frequencies\n",
    "        print(\"\\nStep 1: Calculating term frequencies...\")\n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            term_freq = Counter(tokens)\n",
    "            self.term_frequencies[doc_id] = term_freq\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Processed {doc_id + 1}/{len(self.collection.processed_docs)} documents...\")\n",
    "        \n",
    "        # Step 2: Calculate IDF values\n",
    "        print(\"\\nStep 2: Calculating IDF values...\")\n",
    "        all_terms = self.index.index.keys()\n",
    "        for term in all_terms:\n",
    "            self.idf_values[term] = self.calculate_idf(term)\n",
    "        \n",
    "        print(f\"✓ Calculated IDF for {len(self.idf_values)} unique terms\")\n",
    "        \n",
    "        # Step 3: Calculate TF-IDF scores\n",
    "        print(\"\\nStep 3: Calculating TF-IDF scores...\")\n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            total_terms = doc['token_count']\n",
    "            \n",
    "            self.tfidf_matrix[doc_id] = {}\n",
    "            \n",
    "            for term, count in self.term_frequencies[doc_id].items():\n",
    "                tf = self.calculate_tf(count, total_terms, method='normalized')\n",
    "                idf = self.idf_values.get(term, 0.0)\n",
    "                tfidf = tf * idf\n",
    "                \n",
    "                self.tfidf_matrix[doc_id][term] = tfidf\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Calculated TF-IDF for {doc_id + 1}/{len(self.collection.processed_docs)} documents...\")\n",
    "        \n",
    "        print(\"\\n✓ TF-IDF matrix built successfully!\")\n",
    "        self.display_tfidf_statistics()\n",
    "    \n",
    "    def display_tfidf_statistics(self):\n",
    "        \"\"\"Display TF-IDF statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TF-IDF STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        sorted_idf = sorted(self.idf_values.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 10 Most Discriminative Terms (Highest IDF):\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, (term, idf) in enumerate(sorted_idf[:10], 1):\n",
    "            doc_count = self.index.get_term_frequency(term)\n",
    "            print(f\"{i:2d}. '{term}' - IDF: {idf:.4f} (in {doc_count} docs)\")\n",
    "        \n",
    "        print(\"\\nTop 10 Most Common Terms (Lowest IDF):\")\n",
    "        print(\"-\" * 60)\n",
    "        for i, (term, idf) in enumerate(sorted_idf[-10:][::-1], 1):\n",
    "            doc_count = self.index.get_term_frequency(term)\n",
    "            print(f\"{i:2d}. '{term}' - IDF: {idf:.4f} (in {doc_count} docs)\")\n",
    "    \n",
    "    def calculate_query_tfidf(self, query_terms: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate TF-IDF for query terms\"\"\"\n",
    "        query_tfidf = {}\n",
    "        query_tf = Counter(query_terms)\n",
    "        total_query_terms = len(query_terms)\n",
    "        \n",
    "        for term, count in query_tf.items():\n",
    "            tf = self.calculate_tf(count, total_query_terms, method='normalized')\n",
    "            idf = self.idf_values.get(term, 0.0)\n",
    "            query_tfidf[term] = tf * idf\n",
    "        \n",
    "        return query_tfidf\n",
    "    \n",
    "    def calculate_similarity(self, query_tfidf: Dict[str, float], doc_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate similarity between query and document\n",
    "        Uses dot product of TF-IDF vectors\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "        doc_tfidf = self.tfidf_matrix.get(doc_id, {})\n",
    "        \n",
    "        for term, query_score in query_tfidf.items():\n",
    "            doc_score = doc_tfidf.get(term, 0.0)\n",
    "            score += query_score * doc_score\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for documents using TF-IDF ranking\n",
    "        \n",
    "        Returns ranked list of documents with scores\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"EXECUTING TF-IDF QUERY: '{query}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Preprocess query\n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        print(f\"\\nPreprocessed query: {query_terms}\")\n",
    "        \n",
    "        if not query_terms:\n",
    "            print(\"✗ No valid query terms\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate query TF-IDF\n",
    "        query_tfidf = self.calculate_query_tfidf(query_terms)\n",
    "        \n",
    "        print(\"\\nQuery TF-IDF scores:\")\n",
    "        for term, score in query_tfidf.items():\n",
    "            print(f\"  '{term}': {score:.4f}\")\n",
    "        \n",
    "        # Calculate similarity for all documents\n",
    "        print(\"\\nRanking documents...\")\n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id in range(len(self.collection.documents)):\n",
    "            score = self.calculate_similarity(query_tfidf, doc_id)\n",
    "            if score > 0:\n",
    "                doc_scores.append((doc_id, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"✓ Found {len(doc_scores)} relevant documents\")\n",
    "        \n",
    "        # Get top-k results\n",
    "        results = []\n",
    "        for doc_id, score in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            \n",
    "            # Get title\n",
    "            title = \"N/A\"\n",
    "            if 'Heading' in doc['original']:\n",
    "                title = doc['original']['Heading']\n",
    "            elif 'heading' in doc['original']:\n",
    "                title = doc['original']['heading']\n",
    "            elif 'title' in doc['original']:\n",
    "                title = doc['original']['title']\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:200] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DEMONSTRATION\n",
    "# ============================================================\n",
    "\n",
    "def demonstrate_tfidf():\n",
    "    \"\"\"Complete demonstration of TF-IDF Model\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 3\")\n",
    "    print(\"TF-IDF RETRIEVAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load documents\n",
    "    print(\"\\nStep 1: Loading documents...\")\n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    # Build inverted index\n",
    "    print(\"\\nStep 2: Building inverted index...\")\n",
    "    inverted_index = InvertedIndex()\n",
    "    inverted_index.build_index(collection.processed_docs)\n",
    "    \n",
    "    # Initialize TF-IDF Model\n",
    "    print(\"\\nStep 3: Initializing TF-IDF Model...\")\n",
    "    tfidf_model = TFIDFModel(collection, inverted_index)\n",
    "    \n",
    "    # Demonstrate queries\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUERY DEMONSTRATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    queries = [\n",
    "        \"karachi transport government\",\n",
    "        \"pakistan cricket victory\",\n",
    "        \"lahore education university\",\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        results = tfidf_model.search(query, top_k=5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No results found.\")\n",
    "            continue\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\n{result['rank']}. [Score: {result['score']:.4f}] Doc ID: {result['doc_id']}\")\n",
    "            print(f\"   Title: {result['title']}\")\n",
    "            print(f\"   Preview: {result['text_preview']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 3 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nKey Achievements:\")\n",
    "    print(\"✓ Implemented TF-IDF scoring\")\n",
    "    print(\"✓ Built TF-IDF matrix for all documents\")\n",
    "    print(\"✓ Implemented document ranking\")\n",
    "    print(\"\\nAdvantages of TF-IDF:\")\n",
    "    print(\"✓ Ranked results (most relevant first)\")\n",
    "    print(\"✓ Handles term importance (rare terms weighted higher)\")\n",
    "    print(\"✓ Better user experience than Boolean model\")\n",
    "    print(\"✓ More flexible queries\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "217f6d46-6535-4477-a4a2-e597ce507fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NLTK resources...\n",
      "✓ NLTK resources ready\n",
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 4\n",
      "BM25 RETRIEVAL MODEL\n",
      "============================================================\n",
      "\n",
      "Step 1: Loading documents...\n",
      "✓ Loaded with latin-1 encoding\n",
      "✓ Loaded 2692 documents\n",
      "\n",
      "Preprocessing documents...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "✓ All 2692 documents preprocessed!\n",
      "\n",
      "Step 2: Building inverted index...\n",
      "\n",
      "Building inverted index...\n",
      "  Indexed 500/2692 documents...\n",
      "  Indexed 1000/2692 documents...\n",
      "  Indexed 1500/2692 documents...\n",
      "  Indexed 2000/2692 documents...\n",
      "  Indexed 2500/2692 documents...\n",
      "✓ Indexed 18196 unique terms\n",
      "\n",
      "Step 3: Initializing BM25 Model...\n",
      "\n",
      "✓ BM25 Model initialized\n",
      "  Parameters: k1=1.5, b=0.75\n",
      "\n",
      "============================================================\n",
      "CALCULATING DOCUMENT STATISTICS\n",
      "============================================================\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "\n",
      "✓ Statistics calculated\n",
      "  Total documents: 2692\n",
      "  Average document length: 186.33 terms\n",
      "  Shortest document: 29 terms\n",
      "  Longest document: 1809 terms\n",
      "\n",
      "============================================================\n",
      "CALCULATING BM25 IDF VALUES\n",
      "============================================================\n",
      "✓ Calculated IDF for 18196 unique terms\n",
      "\n",
      "------------------------------------------------------------\n",
      "SAMPLE IDF VALUES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 10 Most Discriminative Terms (Highest IDF):\n",
      " 1. 'rej' - IDF: 7.4929 (in 1 docs)\n",
      " 2. 'rickshaw' - IDF: 7.4929 (in 1 docs)\n",
      " 3. 'kti' - IDF: 7.4929 (in 1 docs)\n",
      " 4. 'tambangraya' - IDF: 7.4929 (in 1 docs)\n",
      " 5. 'megah' - IDF: 7.4929 (in 1 docs)\n",
      " 6. 'cnr' - IDF: 7.4929 (in 1 docs)\n",
      " 7. 'vank' - IDF: 7.4929 (in 1 docs)\n",
      " 8. 'csr' - IDF: 7.4929 (in 1 docs)\n",
      " 9. 'wilmar' - IDF: 7.4929 (in 1 docs)\n",
      "10. 'moribund' - IDF: 7.4929 (in 1 docs)\n",
      "\n",
      "Top 10 Most Common Terms (Lowest IDF):\n",
      " 1. 'said' - IDF: 0.4076 (in 1791 docs)\n",
      " 2. 'strong' - IDF: 0.4938 (in 1643 docs)\n",
      " 3. 'year' - IDF: 0.6098 (in 1463 docs)\n",
      " 4. 'first' - IDF: 0.7719 (in 1244 docs)\n",
      " 5. 'also' - IDF: 0.7759 (in 1239 docs)\n",
      " 6. 'last' - IDF: 0.8196 (in 1186 docs)\n",
      " 7. 'two' - IDF: 0.8618 (in 1137 docs)\n",
      " 8. 'one' - IDF: 0.8627 (in 1136 docs)\n",
      " 9. 'world' - IDF: 0.8769 (in 1120 docs)\n",
      "10. 'day' - IDF: 1.0134 (in 977 docs)\n",
      "\n",
      "============================================================\n",
      "QUERY DEMONSTRATIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXECUTING BM25 QUERY: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['karachi', 'transport', 'govern']\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 754 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "1. [BM25: 16.1509] Doc 0\n",
      "   sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 pe...\n",
      "\n",
      "2. [BM25: 11.8443] Doc 124\n",
      "   goods transport by cargo trains goes up 10 ti\n",
      "   goods transport by cargo trains goes up 10 ti KARACHI: The transportation of cargo through freight trains has witnessed a marked improvement.According...\n",
      "\n",
      "3. [BM25: 10.9975] Doc 2477\n",
      "   Pakistan shippers fume as containers hijacked to block pr\n",
      "   Pakistan shippers fume as containers hijacked to block pr strong>ISLAMABAD: Pakistani business leaders complained Wednesday of losing millions of doll...\n",
      "\n",
      "4. [BM25: 10.9746] Doc 2562\n",
      "   Iran offers to provide buses for public transport sy\n",
      "   Iran offers to provide buses for public transport sy strong>ISLAMABAD: Iran has offered to provide modern buses to improve public transport in Pakista...\n",
      "\n",
      "5. [BM25: 10.6193] Doc 2530\n",
      "   Pakistan China commence direct rail and sea freight servi\n",
      "   Pakistan China commence direct rail and sea freight servi strong>ISLAMABAD/BEIJING: Pakistan and China embarked on their ambitious and effective proje...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BM25 QUERY: 'pakistan cricket match'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['pakistan', 'cricket', 'match']\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 1519 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'pakistan cricket match'\n",
      "============================================================\n",
      "\n",
      "1. [BM25: 7.5991] Doc 1284\n",
      "   Pakistan keen to build on Super League su\n",
      "   Pakistan keen to build on Super League su strong>ISLAMABAD: Pakistan´s inaugural national cricket league has been an unexpected success, even though a...\n",
      "\n",
      "2. [BM25: 7.5926] Doc 2197\n",
      "   Pakistan team black arm bands mourning Edhis d\n",
      "   Pakistan team black arm bands mourning Edhis d strong>LONDON: The Pakistan cricket team will play a match against Sussex on Saturday wearing black arm...\n",
      "\n",
      "3. [BM25: 7.5848] Doc 1349\n",
      "   Rajiv Shukla assures full security for Pakistan team in Dhar\n",
      "   Rajiv Shukla assures full security for Pakistan team in Dhar strong>NEW DELHI: Senior official of the Board of Cricket Control in India (BCCI) Rajiv S...\n",
      "\n",
      "4. [BM25: 7.5540] Doc 2021\n",
      "   Pakistan cricket team leaves for England\n",
      "   Pakistan cricket team leaves for England LAHORE: Pakistan cricket team has left here for England where it will play a series of four Test matches, fol...\n",
      "\n",
      "5. [BM25: 7.5173] Doc 1569\n",
      "   Pakistan beat Sri Lanka in 1st match of Blind cricket seri\n",
      "   Pakistan beat Sri Lanka in 1st match of Blind cricket seri FAISALABAD: Pakistan won the first match of Twenty20 Blind Cricket Series after defeating S...\n",
      "\n",
      "============================================================\n",
      "EXECUTING BM25 QUERY: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['lahor', 'educ', 'univers']\n",
      "\n",
      "Ranking documents...\n",
      "✓ Found 217 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "1. [BM25: 14.9544] Doc 1278\n",
      "   National Women Netball Championship to kick off on March 3\n",
      "   National Women Netball Championship to kick off on March 3 strong>KARACHI: Pakistan Netball Federation (PNF) is set to organize 15th National Women Ne...\n",
      "\n",
      "2. [BM25: 14.3799] Doc 696\n",
      "   Budget HEC to get Rs 21486487 mln for higher educati\n",
      "   Budget HEC to get Rs 21486487 mln for higher educati strong>ISLAMABAD: The government has allocated Rs 21486.487 million for the Higher Education Comm...\n",
      "\n",
      "3. [BM25: 14.0313] Doc 2437\n",
      "   PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year\n",
      "   PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year strong>ISLAMABAD: Prime Minister Nawaz Sharif, Wednesday emphasizing the need to fu...\n",
      "\n",
      "4. [BM25: 12.9153] Doc 1911\n",
      "   Pakistans Diana hunts for glory in cricket and footb\n",
      "   Pakistans Diana hunts for glory in cricket and footb LAHORE: Diana Baig, the talented 20-year-old girl plays for Pakistan´s national team in both cric...\n",
      "\n",
      "5. [BM25: 10.2738] Doc 175\n",
      "   punjab govt presents rs 1.45tr budg\n",
      "   punjab govt presents rs 1.45tr budg LAHORE: The Punjab government on Friday presented a Rs 1,447.2 billion proposed annual budget for the fiscal year ...\n",
      "\n",
      "============================================================\n",
      "BM25 SCORE EXPLANATION\n",
      "============================================================\n",
      "\n",
      "Query: 'karachi transport government'\n",
      "Preprocessed: ['karachi', 'transport', 'govern']\n",
      "\n",
      "Document ID: 0\n",
      "Document length: 82 terms\n",
      "Average length: 186.33 terms\n",
      "Length ratio: 0.44\n",
      "Length normalization: 0.5801\n",
      "\n",
      "------------------------------------------------------------\n",
      "TERM-BY-TERM BREAKDOWN\n",
      "------------------------------------------------------------\n",
      "\n",
      "Term: 'karachi'\n",
      "  TF in doc: 4\n",
      "  DF: 292 docs\n",
      "  IDF: 2.2199\n",
      "  Score: 4.5583\n",
      "\n",
      "Term: 'transport'\n",
      "  TF in doc: 5\n",
      "  DF: 52 docs\n",
      "  IDF: 3.9376\n",
      "  Score: 8.3849\n",
      "\n",
      "Term: 'govern'\n",
      "  TF in doc: 3\n",
      "  DF: 514 docs\n",
      "  IDF: 1.6552\n",
      "  Score: 3.2077\n",
      "\n",
      "------------------------------------------------------------\n",
      "TOTAL BM25 SCORE: 16.1509\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "COMPARING WITH TF-IDF\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON: TF-IDF vs BM25\n",
      "============================================================\n",
      "\n",
      "Query: 'karachi transport government'\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "Building TF-IDF matrix (for comparison)...\n",
      "✓ TF-IDF matrix built\n",
      "\n",
      "✓ BM25 Model initialized\n",
      "  Parameters: k1=1.5, b=0.75\n",
      "\n",
      "============================================================\n",
      "CALCULATING DOCUMENT STATISTICS\n",
      "============================================================\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "\n",
      "✓ Statistics calculated\n",
      "  Total documents: 2692\n",
      "  Average document length: 186.33 terms\n",
      "  Shortest document: 29 terms\n",
      "  Longest document: 1809 terms\n",
      "\n",
      "============================================================\n",
      "CALCULATING BM25 IDF VALUES\n",
      "============================================================\n",
      "✓ Calculated IDF for 18196 unique terms\n",
      "\n",
      "------------------------------------------------------------\n",
      "SAMPLE IDF VALUES\n",
      "------------------------------------------------------------\n",
      "\n",
      "Top 10 Most Discriminative Terms (Highest IDF):\n",
      " 1. 'rej' - IDF: 7.4929 (in 1 docs)\n",
      " 2. 'rickshaw' - IDF: 7.4929 (in 1 docs)\n",
      " 3. 'kti' - IDF: 7.4929 (in 1 docs)\n",
      " 4. 'tambangraya' - IDF: 7.4929 (in 1 docs)\n",
      " 5. 'megah' - IDF: 7.4929 (in 1 docs)\n",
      " 6. 'cnr' - IDF: 7.4929 (in 1 docs)\n",
      " 7. 'vank' - IDF: 7.4929 (in 1 docs)\n",
      " 8. 'csr' - IDF: 7.4929 (in 1 docs)\n",
      " 9. 'wilmar' - IDF: 7.4929 (in 1 docs)\n",
      "10. 'moribund' - IDF: 7.4929 (in 1 docs)\n",
      "\n",
      "Top 10 Most Common Terms (Lowest IDF):\n",
      " 1. 'said' - IDF: 0.4076 (in 1791 docs)\n",
      " 2. 'strong' - IDF: 0.4938 (in 1643 docs)\n",
      " 3. 'year' - IDF: 0.6098 (in 1463 docs)\n",
      " 4. 'first' - IDF: 0.7719 (in 1244 docs)\n",
      " 5. 'also' - IDF: 0.7759 (in 1239 docs)\n",
      " 6. 'last' - IDF: 0.8196 (in 1186 docs)\n",
      " 7. 'two' - IDF: 0.8618 (in 1137 docs)\n",
      " 8. 'one' - IDF: 0.8627 (in 1136 docs)\n",
      " 9. 'world' - IDF: 0.8769 (in 1120 docs)\n",
      "10. 'day' - IDF: 1.0134 (in 977 docs)\n",
      "\n",
      "Retrieving results...\n",
      "\n",
      "============================================================\n",
      "TOP 5 RESULTS COMPARISON\n",
      "============================================================\n",
      "\n",
      "Rank   Model      Score        Doc ID     Title                                   \n",
      "--------------------------------------------------------------------------------\n",
      "1      TF-IDF     0.4935       124        goods transport by cargo trains goes up \n",
      "       BM25       16.1509      0          sindh govt decides to cut public transpo\n",
      "\n",
      "2      TF-IDF     0.4621       2562       Iran offers to provide buses for public \n",
      "       BM25       11.8443      124        goods transport by cargo trains goes up \n",
      "\n",
      "3      TF-IDF     0.4269       0          sindh govt decides to cut public transpo\n",
      "       BM25       10.9975      2477       Pakistan shippers fume as containers hij\n",
      "\n",
      "4      TF-IDF     0.1395       2530       Pakistan China commence direct rail and \n",
      "       BM25       10.9746      2562       Iran offers to provide buses for public \n",
      "\n",
      "5      TF-IDF     0.1118       2663       CPEC China approves huge infrastructure \n",
      "       BM25       10.6193      2530       Pakistan China commence direct rail and \n",
      "\n",
      "\n",
      "============================================================\n",
      "ANALYSIS\n",
      "============================================================\n",
      "Documents in both top-5: 4/5\n",
      "\n",
      "Key Differences:\n",
      "  TF-IDF: Linear term frequency, weak length norm\n",
      "  BM25: Saturation effect, strong length norm\n",
      "\n",
      "============================================================\n",
      "PHASE 4 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "✓ Implemented BM25 ranking\n",
      "✓ Term frequency saturation\n",
      "✓ Document length normalization\n",
      "✓ Compared with TF-IDF\n",
      "\n",
      "BM25 is the gold standard for text retrieval!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 4: BM25 RETRIEVAL MODEL\n",
    "=============================================================\n",
    "This implements the BM25 (Best Match 25) ranking function.\n",
    "\n",
    "What is BM25?\n",
    "-------------\n",
    "BM25 is a probabilistic ranking function that improves upon TF-IDF.\n",
    "It's the \"gold standard\" for text retrieval - used by Elasticsearch and Lucene.\n",
    "\n",
    "Key Improvements over TF-IDF:\n",
    "1. SATURATION: Term frequency has diminishing returns\n",
    "2. DOCUMENT LENGTH NORMALIZATION: Fair comparison\n",
    "3. TUNABLE PARAMETERS: Customize for your data\n",
    "\n",
    "Author: Saad Ali\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Checking NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✓ NLTK resources ready\")\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 1, 2, 3 CLASSES (Embedded)\n",
    "# ============================================================\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Handles all text preprocessing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"Manages the entire document collection\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.doc_ids = []\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        try:\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "                    print(f\"✓ Loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read file\")\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df)} documents\")\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                if 'Heading' in df.columns:\n",
    "                    doc_text += str(row['Heading']) + \" \"\n",
    "                elif 'heading' in df.columns:\n",
    "                    doc_text += str(row['heading']) + \" \"\n",
    "                elif 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                \n",
    "                if 'Article' in df.columns:\n",
    "                    doc_text += str(row['Article'])\n",
    "                elif 'article' in df.columns:\n",
    "                    doc_text += str(row['article'])\n",
    "                elif 'content' in df.columns:\n",
    "                    doc_text += str(row['content'])\n",
    "                elif 'text' in df.columns:\n",
    "                    doc_text += str(row['text'])\n",
    "                \n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        print(\"\\nPreprocessing documents...\")\n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"Inverted Index for efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_doc_freq = defaultdict(int)\n",
    "        self.num_documents = 0\n",
    "    \n",
    "    def build_index(self, processed_docs: List[Dict]):\n",
    "        \"\"\"Build the inverted index\"\"\"\n",
    "        print(\"\\nBuilding inverted index...\")\n",
    "        \n",
    "        self.num_documents = len(processed_docs)\n",
    "        \n",
    "        for doc in processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            unique_terms = set(tokens)\n",
    "            \n",
    "            for term in unique_terms:\n",
    "                self.index[term].add(doc_id)\n",
    "                self.term_doc_freq[term] += 1\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Indexed {doc_id + 1}/{self.num_documents} documents...\")\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.index)} unique terms\")\n",
    "    \n",
    "    def get_postings(self, term: str) -> Set[int]:\n",
    "        return self.index.get(term, set())\n",
    "    \n",
    "    def get_term_frequency(self, term: str) -> int:\n",
    "        return self.term_doc_freq.get(term, 0)\n",
    "\n",
    "\n",
    "class TFIDFModel:\n",
    "    \"\"\"TF-IDF Model (needed for comparison)\"\"\"\n",
    "    \n",
    "    def __init__(self, collection: DocumentCollection, inverted_index: InvertedIndex):\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        self.tfidf_matrix = {}\n",
    "        self.idf_values = {}\n",
    "        self.term_frequencies = {}\n",
    "        self.build_tfidf_matrix()\n",
    "    \n",
    "    def calculate_tf(self, term_count: int, total_terms: int) -> float:\n",
    "        if term_count == 0:\n",
    "            return 0.0\n",
    "        return term_count / total_terms if total_terms > 0 else 0.0\n",
    "    \n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        N = self.index.num_documents\n",
    "        df = self.index.get_term_frequency(term)\n",
    "        if df == 0:\n",
    "            return 0.0\n",
    "        return math.log(N / (1 + df))\n",
    "    \n",
    "    def build_tfidf_matrix(self):\n",
    "        print(\"\\nBuilding TF-IDF matrix (for comparison)...\")\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            term_freq = Counter(tokens)\n",
    "            self.term_frequencies[doc_id] = term_freq\n",
    "        \n",
    "        all_terms = self.index.index.keys()\n",
    "        for term in all_terms:\n",
    "            self.idf_values[term] = self.calculate_idf(term)\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            total_terms = doc['token_count']\n",
    "            self.tfidf_matrix[doc_id] = {}\n",
    "            \n",
    "            for term, count in self.term_frequencies[doc_id].items():\n",
    "                tf = self.calculate_tf(count, total_terms)\n",
    "                idf = self.idf_values.get(term, 0.0)\n",
    "                self.tfidf_matrix[doc_id][term] = tf * idf\n",
    "        \n",
    "        print(\"✓ TF-IDF matrix built\")\n",
    "    \n",
    "    def calculate_query_tfidf(self, query_terms: List[str]) -> Dict[str, float]:\n",
    "        query_tfidf = {}\n",
    "        query_tf = Counter(query_terms)\n",
    "        total_query_terms = len(query_terms)\n",
    "        \n",
    "        for term, count in query_tf.items():\n",
    "            tf = self.calculate_tf(count, total_query_terms)\n",
    "            idf = self.idf_values.get(term, 0.0)\n",
    "            query_tfidf[term] = tf * idf\n",
    "        \n",
    "        return query_tfidf\n",
    "    \n",
    "    def calculate_similarity(self, query_tfidf: Dict[str, float], doc_id: int) -> float:\n",
    "        score = 0.0\n",
    "        doc_tfidf = self.tfidf_matrix.get(doc_id, {})\n",
    "        \n",
    "        for term, query_score in query_tfidf.items():\n",
    "            doc_score = doc_tfidf.get(term, 0.0)\n",
    "            score += query_score * doc_score\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        \n",
    "        if not query_terms:\n",
    "            return []\n",
    "        \n",
    "        query_tfidf = self.calculate_query_tfidf(query_terms)\n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id in range(len(self.collection.documents)):\n",
    "            score = self.calculate_similarity(query_tfidf, doc_id)\n",
    "            if score > 0:\n",
    "                doc_scores.append((doc_id, score))\n",
    "        \n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            \n",
    "            title = \"N/A\"\n",
    "            if 'Heading' in doc['original']:\n",
    "                title = doc['original']['Heading']\n",
    "            elif 'heading' in doc['original']:\n",
    "                title = doc['original']['heading']\n",
    "            elif 'title' in doc['original']:\n",
    "                title = doc['original']['title']\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:200] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 4: BM25 MODEL\n",
    "# ============================================================\n",
    "\n",
    "class BM25Model:\n",
    "    \"\"\"\n",
    "    Implements BM25 Ranking Function\n",
    "    \n",
    "    Mathematical Formula:\n",
    "    --------------------\n",
    "    BM25(q, d) = Σ IDF(qi) × (f(qi,d) × (k1 + 1)) / \n",
    "                             (f(qi,d) + k1 × (1 - b + b × |d|/avgdl))\n",
    "    \n",
    "    Where:\n",
    "    - q: query, d: document, qi: ith query term\n",
    "    - f(qi,d): frequency of qi in document d\n",
    "    - |d|: length of document d\n",
    "    - avgdl: average document length\n",
    "    - k1: term frequency saturation (typically 1.2 to 2.0)\n",
    "    - b: length normalization (typically 0.75)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 collection: DocumentCollection, \n",
    "                 inverted_index: InvertedIndex,\n",
    "                 k1: float = 1.5,\n",
    "                 b: float = 0.75):\n",
    "        \"\"\"\n",
    "        Initialize BM25 Model\n",
    "        \n",
    "        Args:\n",
    "            k1: Controls term frequency saturation (1.2 to 2.0)\n",
    "            b: Controls length normalization (0 to 1, default 0.75)\n",
    "        \"\"\"\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        \n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        self.num_documents = len(collection.documents)\n",
    "        self.avgdl = 0.0\n",
    "        self.doc_lengths = {}\n",
    "        self.idf_values = {}\n",
    "        self.term_frequencies = {}\n",
    "        \n",
    "        print(\"\\n✓ BM25 Model initialized\")\n",
    "        print(f\"  Parameters: k1={k1}, b={b}\")\n",
    "        \n",
    "        self.calculate_statistics()\n",
    "        self.calculate_idf_values()\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        \"\"\"Calculate document statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATING DOCUMENT STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_length = 0\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            \n",
    "            doc_length = len(tokens)\n",
    "            self.doc_lengths[doc_id] = doc_length\n",
    "            total_length += doc_length\n",
    "            \n",
    "            self.term_frequencies[doc_id] = Counter(tokens)\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Processed {doc_id + 1}/{self.num_documents} documents...\")\n",
    "        \n",
    "        self.avgdl = total_length / self.num_documents if self.num_documents > 0 else 0\n",
    "        \n",
    "        print(f\"\\n✓ Statistics calculated\")\n",
    "        print(f\"  Total documents: {self.num_documents}\")\n",
    "        print(f\"  Average document length: {self.avgdl:.2f} terms\")\n",
    "        print(f\"  Shortest document: {min(self.doc_lengths.values())} terms\")\n",
    "        print(f\"  Longest document: {max(self.doc_lengths.values())} terms\")\n",
    "    \n",
    "    def calculate_idf_values(self):\n",
    "        \"\"\"Calculate BM25 IDF values\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CALCULATING BM25 IDF VALUES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        all_terms = self.index.index.keys()\n",
    "        \n",
    "        for term in all_terms:\n",
    "            df = self.index.get_term_frequency(term)\n",
    "            # BM25 IDF formula\n",
    "            idf = math.log((self.num_documents - df + 0.5) / (df + 0.5) + 1.0)\n",
    "            self.idf_values[term] = idf\n",
    "        \n",
    "        print(f\"✓ Calculated IDF for {len(self.idf_values)} unique terms\")\n",
    "        self.display_idf_statistics()\n",
    "    \n",
    "    def display_idf_statistics(self):\n",
    "        \"\"\"Display IDF statistics\"\"\"\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"SAMPLE IDF VALUES\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        sorted_idf = sorted(self.idf_values.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nTop 10 Most Discriminative Terms (Highest IDF):\")\n",
    "        for i, (term, idf) in enumerate(sorted_idf[:10], 1):\n",
    "            df = self.index.get_term_frequency(term)\n",
    "            print(f\"{i:2d}. '{term}' - IDF: {idf:.4f} (in {df} docs)\")\n",
    "        \n",
    "        print(\"\\nTop 10 Most Common Terms (Lowest IDF):\")\n",
    "        for i, (term, idf) in enumerate(sorted_idf[-10:][::-1], 1):\n",
    "            df = self.index.get_term_frequency(term)\n",
    "            print(f\"{i:2d}. '{term}' - IDF: {idf:.4f} (in {df} docs)\")\n",
    "    \n",
    "    def calculate_bm25_score(self, query_terms: List[str], doc_id: int) -> float:\n",
    "        \"\"\"Calculate BM25 score for a document\"\"\"\n",
    "        score = 0.0\n",
    "        \n",
    "        doc_length = self.doc_lengths.get(doc_id, 0)\n",
    "        doc_term_freq = self.term_frequencies.get(doc_id, {})\n",
    "        \n",
    "        # Length normalization factor\n",
    "        length_norm = 1 - self.b + self.b * (doc_length / self.avgdl)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            term_freq = doc_term_freq.get(term, 0)\n",
    "            \n",
    "            if term_freq == 0:\n",
    "                continue\n",
    "            \n",
    "            idf = self.idf_values.get(term, 0)\n",
    "            \n",
    "            # BM25 formula\n",
    "            numerator = term_freq * (self.k1 + 1)\n",
    "            denominator = term_freq + self.k1 * length_norm\n",
    "            term_score = idf * (numerator / denominator)\n",
    "            \n",
    "            score += term_score\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10, verbose: bool = True) -> List[Dict]:\n",
    "        \"\"\"Search using BM25 ranking\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"EXECUTING BM25 QUERY: '{query}'\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nPreprocessed query: {query_terms}\")\n",
    "        \n",
    "        if not query_terms:\n",
    "            if verbose:\n",
    "                print(\"✗ No valid query terms\")\n",
    "            return []\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nRanking documents...\")\n",
    "        \n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id in range(self.num_documents):\n",
    "            score = self.calculate_bm25_score(query_terms, doc_id)\n",
    "            \n",
    "            if score > 0:\n",
    "                doc_scores.append((doc_id, score))\n",
    "        \n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Found {len(doc_scores)} relevant documents\")\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            \n",
    "            title = \"N/A\"\n",
    "            if 'Heading' in doc['original']:\n",
    "                title = doc['original']['Heading']\n",
    "            elif 'heading' in doc['original']:\n",
    "                title = doc['original']['heading']\n",
    "            elif 'title' in doc['original']:\n",
    "                title = doc['original']['title']\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def explain_score(self, query: str, doc_id: int):\n",
    "        \"\"\"Explain BM25 score calculation\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"BM25 SCORE EXPLANATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Preprocessed: {query_terms}\")\n",
    "        print(f\"\\nDocument ID: {doc_id}\")\n",
    "        \n",
    "        doc_length = self.doc_lengths.get(doc_id, 0)\n",
    "        doc_term_freq = self.term_frequencies.get(doc_id, {})\n",
    "        \n",
    "        print(f\"Document length: {doc_length} terms\")\n",
    "        print(f\"Average length: {self.avgdl:.2f} terms\")\n",
    "        print(f\"Length ratio: {doc_length/self.avgdl:.2f}\")\n",
    "        \n",
    "        length_norm = 1 - self.b + self.b * (doc_length / self.avgdl)\n",
    "        print(f\"Length normalization: {length_norm:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"TERM-BY-TERM BREAKDOWN\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        total_score = 0.0\n",
    "        \n",
    "        for term in query_terms:\n",
    "            term_freq = doc_term_freq.get(term, 0)\n",
    "            idf = self.idf_values.get(term, 0)\n",
    "            df = self.index.get_term_frequency(term)\n",
    "            \n",
    "            print(f\"\\nTerm: '{term}'\")\n",
    "            print(f\"  TF in doc: {term_freq}\")\n",
    "            print(f\"  DF: {df} docs\")\n",
    "            print(f\"  IDF: {idf:.4f}\")\n",
    "            \n",
    "            if term_freq > 0:\n",
    "                numerator = term_freq * (self.k1 + 1)\n",
    "                denominator = term_freq + self.k1 * length_norm\n",
    "                term_score = idf * (numerator / denominator)\n",
    "                \n",
    "                print(f\"  Score: {term_score:.4f}\")\n",
    "                total_score += term_score\n",
    "            else:\n",
    "                print(f\"  Score: 0.0 (not in doc)\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"TOTAL BM25 SCORE: {total_score:.4f}\")\n",
    "        print(\"-\"*60)\n",
    "\n",
    "\n",
    "def compare_models(collection, inverted_index, query: str):\n",
    "    \"\"\"Compare TF-IDF and BM25\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON: TF-IDF vs BM25\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    print(\"\\nInitializing models...\")\n",
    "    tfidf_model = TFIDFModel(collection, inverted_index)\n",
    "    bm25_model = BM25Model(collection, inverted_index)\n",
    "    \n",
    "    print(\"\\nRetrieving results...\")\n",
    "    tfidf_results = tfidf_model.search(query, top_k=5)\n",
    "    bm25_results = bm25_model.search(query, top_k=5, verbose=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 5 RESULTS COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'Model':<10} {'Score':<12} {'Doc ID':<10} {'Title':<40}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(min(5, len(tfidf_results), len(bm25_results))):\n",
    "        tfidf_r = tfidf_results[i]\n",
    "        bm25_r = bm25_results[i]\n",
    "        \n",
    "        print(f\"{i+1:<6} {'TF-IDF':<10} {tfidf_r['score']:<12.4f} {tfidf_r['doc_id']:<10} {tfidf_r['title'][:40]}\")\n",
    "        print(f\"{'':<6} {'BM25':<10} {bm25_r['score']:<12.4f} {bm25_r['doc_id']:<10} {bm25_r['title'][:40]}\")\n",
    "        print()\n",
    "    \n",
    "    tfidf_doc_ids = set([r['doc_id'] for r in tfidf_results])\n",
    "    bm25_doc_ids = set([r['doc_id'] for r in bm25_results])\n",
    "    common_docs = tfidf_doc_ids & bm25_doc_ids\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Documents in both top-5: {len(common_docs)}/5\")\n",
    "    print(f\"\\nKey Differences:\")\n",
    "    print(\"  TF-IDF: Linear term frequency, weak length norm\")\n",
    "    print(\"  BM25: Saturation effect, strong length norm\")\n",
    "\n",
    "\n",
    "def demonstrate_bm25():\n",
    "    \"\"\"Complete BM25 demonstration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 4\")\n",
    "    print(\"BM25 RETRIEVAL MODEL\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"\n",
    "    \n",
    "    print(\"\\nStep 1: Loading documents...\")\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    print(\"\\nStep 2: Building inverted index...\")\n",
    "    inverted_index = InvertedIndex()\n",
    "    inverted_index.build_index(collection.processed_docs)\n",
    "    \n",
    "    print(\"\\nStep 3: Initializing BM25 Model...\")\n",
    "    bm25_model = BM25Model(collection, inverted_index, k1=1.5, b=0.75)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUERY DEMONSTRATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    queries = [\n",
    "        \"karachi transport government\",\n",
    "        \"pakistan cricket match\",\n",
    "        \"lahore education university\",\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        results = bm25_model.search(query, top_k=5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\n{result['rank']}. [BM25: {result['score']:.4f}] Doc {result['doc_id']}\")\n",
    "            print(f\"   {result['title']}\")\n",
    "            print(f\"   {result['text_preview']}\")\n",
    "    \n",
    "    # Explain first result\n",
    "    if len(queries) > 0:\n",
    "        results = bm25_model.search(queries[0], top_k=1, verbose=False)\n",
    "        if results:\n",
    "            bm25_model.explain_score(queries[0], results[0]['doc_id'])\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING WITH TF-IDF\")\n",
    "    print(\"=\"*60)\n",
    "    compare_models(collection, inverted_index, queries[0])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 4 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n✓ Implemented BM25 ranking\")\n",
    "    print(\"✓ Term frequency saturation\")\n",
    "    print(\"✓ Document length normalization\")\n",
    "    print(\"✓ Compared with TF-IDF\")\n",
    "    print(\"\\nBM25 is the gold standard for text retrieval!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a82c8bb8-c88b-4f0f-8c87-00734145782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking NLTK resources...\n",
      "✓ NLTK resources ready\n",
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 5\n",
      "VECTOR SPACE MODEL WITH COSINE SIMILARITY\n",
      "============================================================\n",
      "\n",
      "Loading documents...\n",
      "✓ Loaded with latin-1 encoding\n",
      "✓ Loaded 2692 documents\n",
      "\n",
      "Preprocessing documents...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "✓ All 2692 documents preprocessed!\n",
      "\n",
      "Building inverted index...\n",
      "\n",
      "Building inverted index...\n",
      "  Indexed 500/2692 documents...\n",
      "  Indexed 1000/2692 documents...\n",
      "  Indexed 1500/2692 documents...\n",
      "  Indexed 2000/2692 documents...\n",
      "  Indexed 2500/2692 documents...\n",
      "✓ Indexed 18196 unique terms\n",
      "\n",
      "Initializing Vector Space Model...\n",
      "\n",
      "✓ Vector Space Model initialized\n",
      "\n",
      "============================================================\n",
      "BUILDING DOCUMENT VECTORS\n",
      "============================================================\n",
      "\n",
      "Step 1: Calculating term frequencies...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "\n",
      "✓ Vocabulary size: 18196 unique terms\n",
      "\n",
      "Step 2: Calculating IDF values...\n",
      "✓ Calculated IDF for 18196 terms\n",
      "\n",
      "Step 3: Building TF-IDF vectors...\n",
      "  Built vectors for 500/2692 documents...\n",
      "  Built vectors for 1000/2692 documents...\n",
      "  Built vectors for 1500/2692 documents...\n",
      "  Built vectors for 2000/2692 documents...\n",
      "  Built vectors for 2500/2692 documents...\n",
      "\n",
      "✓ Document vectors built successfully!\n",
      "\n",
      "============================================================\n",
      "VECTOR SPACE STATISTICS\n",
      "============================================================\n",
      "\n",
      "Vocabulary (dimensions): 18196\n",
      "Avg non-zero dims per doc: 124.74\n",
      "Vector sparsity: 99.31%\n",
      "\n",
      "Document magnitudes:\n",
      "  Average: 0.4982\n",
      "  Min: 0.2455\n",
      "  Max: 1.3596\n",
      "\n",
      "============================================================\n",
      "QUERY DEMONSTRATIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXECUTING VSM QUERY: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['karachi', 'transport', 'govern']\n",
      "\n",
      "Query vector (top terms):\n",
      "  'transport': 1.6427\n",
      "  'karachi': 1.0727\n",
      "  'govern': 0.8847\n",
      "\n",
      "Calculating cosine similarities...\n",
      "✓ Found 754 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'karachi transport government'\n",
      "============================================================\n",
      "\n",
      "1. [Cosine: 0.4513] Doc 2562\n",
      "   Iran offers to provide buses for public transport sy\n",
      "   Iran offers to provide buses for public transport sy strong>ISLAMABAD: Iran has offered to provide modern buses to improve public transport in Pakista...\n",
      "\n",
      "2. [Cosine: 0.4407] Doc 124\n",
      "   goods transport by cargo trains goes up 10 ti\n",
      "   goods transport by cargo trains goes up 10 ti KARACHI: The transportation of cargo through freight trains has witnessed a marked improvement.According...\n",
      "\n",
      "3. [Cosine: 0.4391] Doc 0\n",
      "   sindh govt decides to cut public transport fares by 7pc kti rej\n",
      "   sindh govt decides to cut public transport fares by 7pc kti rej KARACHI: The Sindh government has decided to bring down public transport fares by 7 pe...\n",
      "\n",
      "4. [Cosine: 0.2167] Doc 2530\n",
      "   Pakistan China commence direct rail and sea freight servi\n",
      "   Pakistan China commence direct rail and sea freight servi strong>ISLAMABAD/BEIJING: Pakistan and China embarked on their ambitious and effective proje...\n",
      "\n",
      "5. [Cosine: 0.1812] Doc 1175\n",
      "   Karachi restrict Islamabad to 132 8\n",
      "   Karachi restrict Islamabad to 132 8 DUBAI: Karachi Kings restricted to Islamabad United to 132 for eight in 20 overs in the sixth match of the Pakista...\n",
      "\n",
      "============================================================\n",
      "EXECUTING VSM QUERY: 'pakistan cricket match'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['pakistan', 'cricket', 'match']\n",
      "\n",
      "Query vector (top terms):\n",
      "  'cricket': 0.8361\n",
      "  'match': 0.7253\n",
      "  'pakistan': 0.6824\n",
      "\n",
      "Calculating cosine similarities...\n",
      "✓ Found 1519 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'pakistan cricket match'\n",
      "============================================================\n",
      "\n",
      "1. [Cosine: 0.3645] Doc 1284\n",
      "   Pakistan keen to build on Super League su\n",
      "   Pakistan keen to build on Super League su strong>ISLAMABAD: Pakistan´s inaugural national cricket league has been an unexpected success, even though a...\n",
      "\n",
      "2. [Cosine: 0.3623] Doc 2230\n",
      "   Afridi questions presence of cricketing talent in Paki\n",
      "   Afridi questions presence of cricketing talent in Paki strong>LONDON: Former Pakistani cricket team captain and swashbuckling batsman Shahid Afridi sa...\n",
      "\n",
      "3. [Cosine: 0.3614] Doc 2007\n",
      "   Misbah England series tests Pakistan cri\n",
      "   Misbah England series tests Pakistan cri strong>LAHORE: Pakistan cricket captain Misbah-ul Haq hopes the upcoming test series in England will help spu...\n",
      "\n",
      "4. [Cosine: 0.3499] Doc 2021\n",
      "   Pakistan cricket team leaves for England\n",
      "   Pakistan cricket team leaves for England LAHORE: Pakistan cricket team has left here for England where it will play a series of four Test matches, fol...\n",
      "\n",
      "5. [Cosine: 0.3490] Doc 1482\n",
      "   Imran congratulates womens team criticizes Pakistan cricket structur\n",
      "   Imran congratulates womens team criticizes Pakistan cricket structur strong>ISLAMABAD: Former captain Imran Khan on Sunday congratulated Pakistan Wome...\n",
      "\n",
      "============================================================\n",
      "EXECUTING VSM QUERY: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "Preprocessed query: ['lahor', 'educ', 'univers']\n",
      "\n",
      "Query vector (top terms):\n",
      "  'univers': 1.7202\n",
      "  'educ': 1.6070\n",
      "  'lahor': 1.3213\n",
      "\n",
      "Calculating cosine similarities...\n",
      "✓ Found 217 relevant documents\n",
      "\n",
      "============================================================\n",
      "Query: 'lahore education university'\n",
      "============================================================\n",
      "\n",
      "1. [Cosine: 0.2501] Doc 1104\n",
      "   Green shirts Kiwis observe one minute silence for university martyr\n",
      "   Green shirts Kiwis observe one minute silence for university martyr strong>WELLINGTON: Pakistan and New Zealand teams observed one-minute silence in t...\n",
      "\n",
      "2. [Cosine: 0.2239] Doc 2437\n",
      "   PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year\n",
      "   PM for enhancing bilateral trade with Belarus upto 1 bln in 4 year strong>ISLAMABAD: Prime Minister Nawaz Sharif, Wednesday emphasizing the need to fu...\n",
      "\n",
      "3. [Cosine: 0.2193] Doc 696\n",
      "   Budget HEC to get Rs 21486487 mln for higher educati\n",
      "   Budget HEC to get Rs 21486487 mln for higher educati strong>ISLAMABAD: The government has allocated Rs 21486.487 million for the Higher Education Comm...\n",
      "\n",
      "4. [Cosine: 0.1679] Doc 1178\n",
      "   PSL Peshawar Zalmi tames Lahore Qalandars with 9 wi\n",
      "   PSL Peshawar Zalmi tames Lahore Qalandars with 9 wi LAHORE: Peshawar Zalmi secured a second consecutive win by defeating Lahore Qalandars by 9 wickets...\n",
      "\n",
      "5. [Cosine: 0.1640] Doc 1212\n",
      "   Bopara steers Karachi Kings to 178 against Lahore Qalandar\n",
      "   Bopara steers Karachi Kings to 178 against Lahore Qalandar SHARJAH: Ravi Bopara smashed an unbeaten 71 off 43 balls to take Karachi Kings to 178 for f...\n",
      "\n",
      "============================================================\n",
      "SIMILARITY EXPLANATION EXAMPLE\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COSINE SIMILARITY EXPLANATION\n",
      "============================================================\n",
      "\n",
      "Query: 'karachi transport government'\n",
      "Preprocessed: ['karachi', 'transport', 'govern']\n",
      "Document ID: 2562\n",
      "\n",
      "------------------------------------------------------------\n",
      "QUERY VECTOR:\n",
      "  'transport': 1.6427\n",
      "  'karachi': 1.0727\n",
      "  'govern': 0.8847\n",
      "\n",
      "DOCUMENT VECTOR (matching terms):\n",
      "  'karachi': 0.0000\n",
      "  'transport': 0.4332\n",
      "  'govern': 0.0292\n",
      "\n",
      "------------------------------------------------------------\n",
      "CALCULATION:\n",
      "  Dot product (q · d): 0.7375\n",
      "  Query magnitude ||q||: 2.1522\n",
      "  Doc magnitude ||d||: 0.7594\n",
      "  Cosine similarity: 0.7375 / (2.1522 × 0.7594)\n",
      "  Result: 0.4513\n",
      "\n",
      "------------------------------------------------------------\n",
      "○ Moderate similarity (0.4513) - Somewhat relevant\n",
      "\n",
      "============================================================\n",
      "COMPARING ALL MODELS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "MODEL COMPARISON: TF-IDF vs BM25 vs VSM\n",
      "============================================================\n",
      "\n",
      "Query: 'karachi transport government'\n",
      "\n",
      "Initializing models...\n",
      "\n",
      "Building TF-IDF matrix (for comparison)...\n",
      "✓ TF-IDF matrix built\n",
      "\n",
      "Calculating BM25 statistics...\n",
      "✓ BM25 statistics calculated (avgdl: 186.33)\n",
      "\n",
      "✓ Vector Space Model initialized\n",
      "\n",
      "============================================================\n",
      "BUILDING DOCUMENT VECTORS\n",
      "============================================================\n",
      "\n",
      "Step 1: Calculating term frequencies...\n",
      "  Processed 500/2692 documents...\n",
      "  Processed 1000/2692 documents...\n",
      "  Processed 1500/2692 documents...\n",
      "  Processed 2000/2692 documents...\n",
      "  Processed 2500/2692 documents...\n",
      "\n",
      "✓ Vocabulary size: 18196 unique terms\n",
      "\n",
      "Step 2: Calculating IDF values...\n",
      "✓ Calculated IDF for 18196 terms\n",
      "\n",
      "Step 3: Building TF-IDF vectors...\n",
      "  Built vectors for 500/2692 documents...\n",
      "  Built vectors for 1000/2692 documents...\n",
      "  Built vectors for 1500/2692 documents...\n",
      "  Built vectors for 2000/2692 documents...\n",
      "  Built vectors for 2500/2692 documents...\n",
      "\n",
      "✓ Document vectors built successfully!\n",
      "\n",
      "============================================================\n",
      "VECTOR SPACE STATISTICS\n",
      "============================================================\n",
      "\n",
      "Vocabulary (dimensions): 18196\n",
      "Avg non-zero dims per doc: 124.74\n",
      "Vector sparsity: 99.31%\n",
      "\n",
      "Document magnitudes:\n",
      "  Average: 0.4982\n",
      "  Min: 0.2455\n",
      "  Max: 1.3596\n",
      "\n",
      "Retrieving results...\n",
      "\n",
      "============================================================\n",
      "TOP 5 RESULTS\n",
      "============================================================\n",
      "\n",
      "Rank   TF-IDF       BM25         VSM (Cosine)   \n",
      "--------------------------------------------------\n",
      "1         0.4935     16.1509         0.4513\n",
      "2         0.4621     11.8443         0.4407\n",
      "3         0.4269     10.9975         0.4391\n",
      "4         0.1395     10.9746         0.2167\n",
      "5         0.1118     10.6193         0.1812\n",
      "\n",
      "============================================================\n",
      "MODEL CHARACTERISTICS\n",
      "============================================================\n",
      "\n",
      "TF-IDF: Simple, linear, no normalization\n",
      "BM25: Saturation, length norm, industry standard\n",
      "VSM: Geometric, angle-based, intuitive\n",
      "\n",
      "============================================================\n",
      "PHASE 5 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "✓ Implemented Vector Space Model\n",
      "✓ Represented documents as vectors\n",
      "✓ Implemented cosine similarity\n",
      "✓ Compared all three retrieval models\n",
      "\n",
      "VSM Advantages:\n",
      "✓ Geometric interpretation (intuitive)\n",
      "✓ Length-independent by design\n",
      "✓ Solid theoretical foundation\n",
      "✓ Extensible to clustering & classification\n",
      "\n",
      "You now have 3 powerful retrieval models!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 5: VECTOR SPACE MODEL\n",
    "===========================================================\n",
    "This implements the Vector Space Model (VSM) with Cosine Similarity.\n",
    "\n",
    "What is the Vector Space Model?\n",
    "--------------------------------\n",
    "VSM represents documents and queries as vectors in a high-dimensional space,\n",
    "where each dimension corresponds to a term in the vocabulary.\n",
    "\n",
    "Key Concept: GEOMETRIC INTERPRETATION\n",
    "- Documents are points in space\n",
    "- Similar documents are close together\n",
    "- We measure \"closeness\" using angles (cosine similarity)\n",
    "\n",
    "Why Cosine Similarity?\n",
    "----------------------\n",
    "Cosine measures the ANGLE between vectors, not distance.\n",
    "- Angle of 0° (cos=1): Identical documents\n",
    "- Angle of 90° (cos=0): Unrelated documents\n",
    "- Length-independent: Fair comparison of all document sizes\n",
    "\n",
    "Author: Saad Ali\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List, Dict, Tuple, Set\n",
    "\n",
    "# Download required NLTK data\n",
    "print(\"Checking NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✓ NLTK resources ready\")\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt_tab')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PREVIOUS PHASES (Embedded)\n",
    "# ============================================================\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"Handles all text preprocessing tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"Manages the entire document collection\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []\n",
    "        self.processed_docs = []\n",
    "        self.doc_ids = []\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        try:\n",
    "            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "            df = None\n",
    "            \n",
    "            for encoding in encodings_to_try:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_path, encoding=encoding)\n",
    "                    print(f\"✓ Loaded with {encoding} encoding\")\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                raise ValueError(\"Could not read file\")\n",
    "            \n",
    "            print(f\"✓ Loaded {len(df)} documents\")\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                if 'Heading' in df.columns:\n",
    "                    doc_text += str(row['Heading']) + \" \"\n",
    "                elif 'heading' in df.columns:\n",
    "                    doc_text += str(row['heading']) + \" \"\n",
    "                elif 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                \n",
    "                if 'Article' in df.columns:\n",
    "                    doc_text += str(row['Article'])\n",
    "                elif 'article' in df.columns:\n",
    "                    doc_text += str(row['article'])\n",
    "                elif 'content' in df.columns:\n",
    "                    doc_text += str(row['content'])\n",
    "                elif 'text' in df.columns:\n",
    "                    doc_text += str(row['text'])\n",
    "                \n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        print(\"\\nPreprocessing documents...\")\n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "class InvertedIndex:\n",
    "    \"\"\"Inverted Index for efficient retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_doc_freq = defaultdict(int)\n",
    "        self.num_documents = 0\n",
    "    \n",
    "    def build_index(self, processed_docs: List[Dict]):\n",
    "        print(\"\\nBuilding inverted index...\")\n",
    "        \n",
    "        self.num_documents = len(processed_docs)\n",
    "        \n",
    "        for doc in processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            unique_terms = set(tokens)\n",
    "            \n",
    "            for term in unique_terms:\n",
    "                self.index[term].add(doc_id)\n",
    "                self.term_doc_freq[term] += 1\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Indexed {doc_id + 1}/{self.num_documents} documents...\")\n",
    "        \n",
    "        print(f\"✓ Indexed {len(self.index)} unique terms\")\n",
    "    \n",
    "    def get_postings(self, term: str) -> Set[int]:\n",
    "        return self.index.get(term, set())\n",
    "    \n",
    "    def get_term_frequency(self, term: str) -> int:\n",
    "        return self.term_doc_freq.get(term, 0)\n",
    "\n",
    "\n",
    "class TFIDFModel:\n",
    "    \"\"\"TF-IDF Model (for comparison)\"\"\"\n",
    "    \n",
    "    def __init__(self, collection: DocumentCollection, inverted_index: InvertedIndex):\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        self.tfidf_matrix = {}\n",
    "        self.idf_values = {}\n",
    "        self.term_frequencies = {}\n",
    "        self.build_tfidf_matrix()\n",
    "    \n",
    "    def calculate_tf(self, term_count: int, total_terms: int) -> float:\n",
    "        if term_count == 0:\n",
    "            return 0.0\n",
    "        return term_count / total_terms if total_terms > 0 else 0.0\n",
    "    \n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        N = self.index.num_documents\n",
    "        df = self.index.get_term_frequency(term)\n",
    "        if df == 0:\n",
    "            return 0.0\n",
    "        return math.log(N / (1 + df))\n",
    "    \n",
    "    def build_tfidf_matrix(self):\n",
    "        print(\"\\nBuilding TF-IDF matrix (for comparison)...\")\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            self.term_frequencies[doc_id] = Counter(doc['tokens'])\n",
    "        \n",
    "        for term in self.index.index.keys():\n",
    "            self.idf_values[term] = self.calculate_idf(term)\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            total_terms = doc['token_count']\n",
    "            self.tfidf_matrix[doc_id] = {}\n",
    "            \n",
    "            for term, count in self.term_frequencies[doc_id].items():\n",
    "                tf = self.calculate_tf(count, total_terms)\n",
    "                idf = self.idf_values.get(term, 0.0)\n",
    "                self.tfidf_matrix[doc_id][term] = tf * idf\n",
    "        \n",
    "        print(\"✓ TF-IDF matrix built\")\n",
    "    \n",
    "    def calculate_query_tfidf(self, query_terms: List[str]) -> Dict[str, float]:\n",
    "        query_tfidf = {}\n",
    "        query_tf = Counter(query_terms)\n",
    "        total = len(query_terms)\n",
    "        \n",
    "        for term, count in query_tf.items():\n",
    "            tf = self.calculate_tf(count, total)\n",
    "            idf = self.idf_values.get(term, 0.0)\n",
    "            query_tfidf[term] = tf * idf\n",
    "        \n",
    "        return query_tfidf\n",
    "    \n",
    "    def calculate_similarity(self, query_tfidf: Dict[str, float], doc_id: int) -> float:\n",
    "        score = 0.0\n",
    "        doc_tfidf = self.tfidf_matrix.get(doc_id, {})\n",
    "        \n",
    "        for term, q_score in query_tfidf.items():\n",
    "            score += q_score * doc_tfidf.get(term, 0.0)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        if not query_terms:\n",
    "            return []\n",
    "        \n",
    "        query_tfidf = self.calculate_query_tfidf(query_terms)\n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id in range(len(self.collection.documents)):\n",
    "            score = self.calculate_similarity(query_tfidf, doc_id)\n",
    "            if score > 0:\n",
    "                doc_scores.append((doc_id, score))\n",
    "        \n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            title = doc['original'].get('Heading', doc['original'].get('title', 'N/A'))\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "class BM25Model:\n",
    "    \"\"\"BM25 Model (for comparison)\"\"\"\n",
    "    \n",
    "    def __init__(self, collection: DocumentCollection, inverted_index: InvertedIndex, k1=1.5, b=0.75):\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.num_documents = len(collection.documents)\n",
    "        self.avgdl = 0.0\n",
    "        self.doc_lengths = {}\n",
    "        self.idf_values = {}\n",
    "        self.term_frequencies = {}\n",
    "        self.calculate_statistics()\n",
    "        self.calculate_idf_values()\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        print(\"\\nCalculating BM25 statistics...\")\n",
    "        total_length = 0\n",
    "        \n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            doc_length = len(doc['tokens'])\n",
    "            self.doc_lengths[doc_id] = doc_length\n",
    "            total_length += doc_length\n",
    "            self.term_frequencies[doc_id] = Counter(doc['tokens'])\n",
    "        \n",
    "        self.avgdl = total_length / self.num_documents if self.num_documents > 0 else 0\n",
    "        print(f\"✓ BM25 statistics calculated (avgdl: {self.avgdl:.2f})\")\n",
    "    \n",
    "    def calculate_idf_values(self):\n",
    "        for term in self.index.index.keys():\n",
    "            df = self.index.get_term_frequency(term)\n",
    "            idf = math.log((self.num_documents - df + 0.5) / (df + 0.5) + 1.0)\n",
    "            self.idf_values[term] = idf\n",
    "    \n",
    "    def calculate_bm25_score(self, query_terms: List[str], doc_id: int) -> float:\n",
    "        score = 0.0\n",
    "        doc_length = self.doc_lengths.get(doc_id, 0)\n",
    "        doc_term_freq = self.term_frequencies.get(doc_id, {})\n",
    "        length_norm = 1 - self.b + self.b * (doc_length / self.avgdl)\n",
    "        \n",
    "        for term in query_terms:\n",
    "            term_freq = doc_term_freq.get(term, 0)\n",
    "            if term_freq == 0:\n",
    "                continue\n",
    "            \n",
    "            idf = self.idf_values.get(term, 0)\n",
    "            numerator = term_freq * (self.k1 + 1)\n",
    "            denominator = term_freq + self.k1 * length_norm\n",
    "            score += idf * (numerator / denominator)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10, verbose: bool = False) -> List[Dict]:\n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        if not query_terms:\n",
    "            return []\n",
    "        \n",
    "        doc_scores = []\n",
    "        for doc_id in range(self.num_documents):\n",
    "            score = self.calculate_bm25_score(query_terms, doc_id)\n",
    "            if score > 0:\n",
    "                doc_scores.append((doc_id, score))\n",
    "        \n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            title = doc['original'].get('Heading', doc['original'].get('title', 'N/A'))\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': score,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 5: VECTOR SPACE MODEL\n",
    "# ============================================================\n",
    "\n",
    "class VectorSpaceModel:\n",
    "    \"\"\"\n",
    "    Implements Vector Space Model with Cosine Similarity\n",
    "    \n",
    "    Mathematical Formula:\n",
    "    --------------------\n",
    "    Cosine Similarity = (q · d) / (||q|| × ||d||)\n",
    "    \n",
    "    Where:\n",
    "    - q · d = dot product (Σ qi × di)\n",
    "    - ||q|| = magnitude of query vector (√Σ qi²)\n",
    "    - ||d|| = magnitude of document vector (√Σ di²)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, collection: DocumentCollection, inverted_index: InvertedIndex):\n",
    "        self.collection = collection\n",
    "        self.index = inverted_index\n",
    "        self.preprocessor = collection.preprocessor\n",
    "        \n",
    "        # Document vectors: {doc_id: {term: weight}}\n",
    "        self.doc_vectors = {}\n",
    "        \n",
    "        # Document magnitudes: {doc_id: magnitude}\n",
    "        self.doc_magnitudes = {}\n",
    "        \n",
    "        # IDF values\n",
    "        self.idf_values = {}\n",
    "        \n",
    "        # Term frequencies\n",
    "        self.term_frequencies = {}\n",
    "        \n",
    "        # Vocabulary\n",
    "        self.vocabulary = set()\n",
    "        \n",
    "        print(\"\\n✓ Vector Space Model initialized\")\n",
    "        self.build_document_vectors()\n",
    "    \n",
    "    def calculate_idf(self, term: str) -> float:\n",
    "        \"\"\"Calculate IDF value\"\"\"\n",
    "        N = self.index.num_documents\n",
    "        df = self.index.get_term_frequency(term)\n",
    "        \n",
    "        if df == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        return math.log((N + 1) / (df + 1)) + 1\n",
    "    \n",
    "    def calculate_tf(self, term_count: int, total_terms: int) -> float:\n",
    "        \"\"\"Calculate normalized TF\"\"\"\n",
    "        if total_terms == 0:\n",
    "            return 0.0\n",
    "        return term_count / total_terms\n",
    "    \n",
    "    def build_document_vectors(self):\n",
    "        \"\"\"Build TF-IDF vectors for all documents\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BUILDING DOCUMENT VECTORS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Calculate term frequencies\n",
    "        print(\"\\nStep 1: Calculating term frequencies...\")\n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            tokens = doc['tokens']\n",
    "            \n",
    "            self.term_frequencies[doc_id] = Counter(tokens)\n",
    "            self.vocabulary.update(tokens)\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Processed {doc_id + 1}/{len(self.collection.processed_docs)} documents...\")\n",
    "        \n",
    "        print(f\"\\n✓ Vocabulary size: {len(self.vocabulary)} unique terms\")\n",
    "        \n",
    "        # Step 2: Calculate IDF values\n",
    "        print(\"\\nStep 2: Calculating IDF values...\")\n",
    "        for term in self.vocabulary:\n",
    "            self.idf_values[term] = self.calculate_idf(term)\n",
    "        \n",
    "        print(f\"✓ Calculated IDF for {len(self.idf_values)} terms\")\n",
    "        \n",
    "        # Step 3: Build TF-IDF vectors\n",
    "        print(\"\\nStep 3: Building TF-IDF vectors...\")\n",
    "        for doc in self.collection.processed_docs:\n",
    "            doc_id = doc['id']\n",
    "            total_terms = doc['token_count']\n",
    "            \n",
    "            vector = {}\n",
    "            \n",
    "            for term, count in self.term_frequencies[doc_id].items():\n",
    "                tf = self.calculate_tf(count, total_terms)\n",
    "                idf = self.idf_values[term]\n",
    "                tfidf = tf * idf\n",
    "                \n",
    "                if tfidf > 0:\n",
    "                    vector[term] = tfidf\n",
    "            \n",
    "            self.doc_vectors[doc_id] = vector\n",
    "            \n",
    "            # Calculate magnitude\n",
    "            magnitude = self.calculate_magnitude(vector)\n",
    "            self.doc_magnitudes[doc_id] = magnitude\n",
    "            \n",
    "            if (doc_id + 1) % 500 == 0:\n",
    "                print(f\"  Built vectors for {doc_id + 1}/{len(self.collection.processed_docs)} documents...\")\n",
    "        \n",
    "        print(\"\\n✓ Document vectors built successfully!\")\n",
    "        self.display_vector_statistics()\n",
    "    \n",
    "    def calculate_magnitude(self, vector: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate vector magnitude: sqrt(Σ wi²)\"\"\"\n",
    "        sum_of_squares = sum(weight ** 2 for weight in vector.values())\n",
    "        return math.sqrt(sum_of_squares)\n",
    "    \n",
    "    def calculate_dot_product(self, vector1: Dict[str, float], vector2: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate dot product: Σ (v1_i × v2_i)\"\"\"\n",
    "        dot_product = 0.0\n",
    "        \n",
    "        # Iterate over smaller vector\n",
    "        if len(vector1) < len(vector2):\n",
    "            smaller, larger = vector1, vector2\n",
    "        else:\n",
    "            smaller, larger = vector2, vector1\n",
    "        \n",
    "        for term, weight1 in smaller.items():\n",
    "            if term in larger:\n",
    "                dot_product += weight1 * larger[term]\n",
    "        \n",
    "        return dot_product\n",
    "    \n",
    "    def calculate_cosine_similarity(self, query_vector: Dict[str, float], doc_id: int) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity\n",
    "        \n",
    "        Cosine = (q · d) / (||q|| × ||d||)\n",
    "        \"\"\"\n",
    "        doc_vector = self.doc_vectors.get(doc_id, {})\n",
    "        doc_magnitude = self.doc_magnitudes.get(doc_id, 0)\n",
    "        \n",
    "        if doc_magnitude == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        query_magnitude = self.calculate_magnitude(query_vector)\n",
    "        \n",
    "        if query_magnitude == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        dot_product = self.calculate_dot_product(query_vector, doc_vector)\n",
    "        cosine_sim = dot_product / (query_magnitude * doc_magnitude)\n",
    "        \n",
    "        return cosine_sim\n",
    "    \n",
    "    def build_query_vector(self, query_terms: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Build TF-IDF vector for query\"\"\"\n",
    "        query_vector = {}\n",
    "        query_tf = Counter(query_terms)\n",
    "        total = len(query_terms)\n",
    "        \n",
    "        for term, count in query_tf.items():\n",
    "            tf = self.calculate_tf(count, total)\n",
    "            idf = self.idf_values.get(term, 0)\n",
    "            tfidf = tf * idf\n",
    "            \n",
    "            if tfidf > 0:\n",
    "                query_vector[term] = tfidf\n",
    "        \n",
    "        return query_vector\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 10, verbose: bool = True) -> List[Dict]:\n",
    "        \"\"\"Search using Vector Space Model with Cosine Similarity\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"EXECUTING VSM QUERY: '{query}'\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nPreprocessed query: {query_terms}\")\n",
    "        \n",
    "        if not query_terms:\n",
    "            if verbose:\n",
    "                print(\"✗ No valid query terms\")\n",
    "            return []\n",
    "        \n",
    "        query_vector = self.build_query_vector(query_terms)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nQuery vector (top terms):\")\n",
    "            sorted_qv = sorted(query_vector.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            for term, weight in sorted_qv:\n",
    "                print(f\"  '{term}': {weight:.4f}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nCalculating cosine similarities...\")\n",
    "        \n",
    "        doc_scores = []\n",
    "        \n",
    "        for doc_id in range(len(self.collection.documents)):\n",
    "            similarity = self.calculate_cosine_similarity(query_vector, doc_id)\n",
    "            \n",
    "            if similarity > 0:\n",
    "                doc_scores.append((doc_id, similarity))\n",
    "        \n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Found {len(doc_scores)} relevant documents\")\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, similarity in doc_scores[:top_k]:\n",
    "            doc = self.collection.get_document(doc_id)\n",
    "            title = doc['original'].get('Heading', doc['original'].get('title', 'N/A'))\n",
    "            \n",
    "            results.append({\n",
    "                'rank': len(results) + 1,\n",
    "                'doc_id': doc_id,\n",
    "                'score': similarity,\n",
    "                'title': title,\n",
    "                'text_preview': doc['text'][:150] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_vector_statistics(self):\n",
    "        \"\"\"Display statistics about vectors\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VECTOR SPACE STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_dims = len(self.vocabulary)\n",
    "        avg_nonzero = np.mean([len(vec) for vec in self.doc_vectors.values()])\n",
    "        sparsity = (1 - avg_nonzero / total_dims) * 100\n",
    "        \n",
    "        print(f\"\\nVocabulary (dimensions): {total_dims}\")\n",
    "        print(f\"Avg non-zero dims per doc: {avg_nonzero:.2f}\")\n",
    "        print(f\"Vector sparsity: {sparsity:.2f}%\")\n",
    "        \n",
    "        magnitudes = list(self.doc_magnitudes.values())\n",
    "        print(f\"\\nDocument magnitudes:\")\n",
    "        print(f\"  Average: {np.mean(magnitudes):.4f}\")\n",
    "        print(f\"  Min: {np.min(magnitudes):.4f}\")\n",
    "        print(f\"  Max: {np.max(magnitudes):.4f}\")\n",
    "    \n",
    "    def explain_similarity(self, query: str, doc_id: int):\n",
    "        \"\"\"Explain cosine similarity calculation\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COSINE SIMILARITY EXPLANATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        query_terms = self.preprocessor.preprocess(query)\n",
    "        query_vector = self.build_query_vector(query_terms)\n",
    "        \n",
    "        print(f\"\\nQuery: '{query}'\")\n",
    "        print(f\"Preprocessed: {query_terms}\")\n",
    "        print(f\"Document ID: {doc_id}\")\n",
    "        \n",
    "        doc_vector = self.doc_vectors.get(doc_id, {})\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"QUERY VECTOR:\")\n",
    "        for term, weight in sorted(query_vector.items(), key=lambda x: x[1], reverse=True):\n",
    "            print(f\"  '{term}': {weight:.4f}\")\n",
    "        \n",
    "        print(\"\\nDOCUMENT VECTOR (matching terms):\")\n",
    "        for term in query_terms:\n",
    "            doc_weight = doc_vector.get(term, 0.0)\n",
    "            print(f\"  '{term}': {doc_weight:.4f}\")\n",
    "        \n",
    "        query_magnitude = self.calculate_magnitude(query_vector)\n",
    "        doc_magnitude = self.doc_magnitudes.get(doc_id, 0)\n",
    "        dot_product = self.calculate_dot_product(query_vector, doc_vector)\n",
    "        cosine_sim = self.calculate_cosine_similarity(query_vector, doc_id)\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"CALCULATION:\")\n",
    "        print(f\"  Dot product (q · d): {dot_product:.4f}\")\n",
    "        print(f\"  Query magnitude ||q||: {query_magnitude:.4f}\")\n",
    "        print(f\"  Doc magnitude ||d||: {doc_magnitude:.4f}\")\n",
    "        print(f\"  Cosine similarity: {dot_product:.4f} / ({query_magnitude:.4f} × {doc_magnitude:.4f})\")\n",
    "        print(f\"  Result: {cosine_sim:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        if cosine_sim >= 0.7:\n",
    "            print(f\"✓ High similarity ({cosine_sim:.4f}) - Very relevant!\")\n",
    "        elif cosine_sim >= 0.3:\n",
    "            print(f\"○ Moderate similarity ({cosine_sim:.4f}) - Somewhat relevant\")\n",
    "        else:\n",
    "            print(f\"✗ Low similarity ({cosine_sim:.4f}) - Not very relevant\")\n",
    "\n",
    "\n",
    "def compare_all_models(collection, inverted_index, query: str):\n",
    "    \"\"\"Compare all models\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON: TF-IDF vs BM25 vs VSM\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    \n",
    "    print(\"\\nInitializing models...\")\n",
    "    tfidf_model = TFIDFModel(collection, inverted_index)\n",
    "    bm25_model = BM25Model(collection, inverted_index)\n",
    "    vsm_model = VectorSpaceModel(collection, inverted_index)\n",
    "    \n",
    "    print(\"\\nRetrieving results...\")\n",
    "    tfidf_results = tfidf_model.search(query, top_k=5)\n",
    "    bm25_results = bm25_model.search(query, top_k=5, verbose=False)\n",
    "    vsm_results = vsm_model.search(query, top_k=5, verbose=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TOP 5 RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\n{'Rank':<6} {'TF-IDF':<12} {'BM25':<12} {'VSM (Cosine)':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(f\"{i+1:<6}\", end=\"\")\n",
    "        \n",
    "        if i < len(tfidf_results):\n",
    "            print(f\"{tfidf_results[i]['score']:>10.4f}  \", end=\"\")\n",
    "        else:\n",
    "            print(f\"{'N/A':>10}  \", end=\"\")\n",
    "        \n",
    "        if i < len(bm25_results):\n",
    "            print(f\"{bm25_results[i]['score']:>10.4f}  \", end=\"\")\n",
    "        else:\n",
    "            print(f\"{'N/A':>10}  \", end=\"\")\n",
    "        \n",
    "        if i < len(vsm_results):\n",
    "            print(f\"{vsm_results[i]['score']:>13.4f}\")\n",
    "        else:\n",
    "            print(f\"{'N/A':>13}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL CHARACTERISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nTF-IDF: Simple, linear, no normalization\")\n",
    "    print(\"BM25: Saturation, length norm, industry standard\")\n",
    "    print(\"VSM: Geometric, angle-based, intuitive\")\n",
    "\n",
    "\n",
    "def demonstrate_vsm():\n",
    "    \"\"\"Complete VSM demonstration\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 5\")\n",
    "    print(\"VECTOR SPACE MODEL WITH COSINE SIMILARITY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"\n",
    "    \n",
    "    print(\"\\nLoading documents...\")\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    print(\"\\nBuilding inverted index...\")\n",
    "    inverted_index = InvertedIndex()\n",
    "    inverted_index.build_index(collection.processed_docs)\n",
    "    \n",
    "    print(\"\\nInitializing Vector Space Model...\")\n",
    "    vsm_model = VectorSpaceModel(collection, inverted_index)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUERY DEMONSTRATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    queries = [\n",
    "        \"karachi transport government\",\n",
    "        \"pakistan cricket match\",\n",
    "        \"lahore education university\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        results = vsm_model.search(query, top_k=5)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Query: '{query}'\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\n{result['rank']}. [Cosine: {result['score']:.4f}] Doc {result['doc_id']}\")\n",
    "            print(f\"   {result['title']}\")\n",
    "            print(f\"   {result['text_preview']}\")\n",
    "    \n",
    "    # Explain similarity\n",
    "    if len(queries) > 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SIMILARITY EXPLANATION EXAMPLE\")\n",
    "        print(\"=\"*60)\n",
    "        results = vsm_model.search(queries[0], top_k=1, verbose=False)\n",
    "        if results:\n",
    "            vsm_model.explain_similarity(queries[0], results[0]['doc_id'])\n",
    "    \n",
    "    # Compare all models\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARING ALL MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    compare_all_models(collection, inverted_index, queries[0])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 5 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n✓ Implemented Vector Space Model\")\n",
    "    print(\"✓ Represented documents as vectors\")\n",
    "    print(\"✓ Implemented cosine similarity\")\n",
    "    print(\"✓ Compared all three retrieval models\")\n",
    "    print(\"\\nVSM Advantages:\")\n",
    "    print(\"✓ Geometric interpretation (intuitive)\")\n",
    "    print(\"✓ Length-independent by design\")\n",
    "    print(\"✓ Solid theoretical foundation\")\n",
    "    print(\"✓ Extensible to clustering & classification\")\n",
    "    print(\"\\nYou now have 3 powerful retrieval models!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_vsm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09240ca2-32df-41fa-ad7e-a267da94aaed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
