{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4fb4b79-0aac-4ec9-a6a0-6d0d4fe6ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INFORMATION RETRIEVAL SYSTEM - PHASE 1\n",
      "DATA LOADING AND PREPROCESSING\n",
      "============================================================\n",
      "✓ Preprocessor initialized\n",
      "✓ Loaded 198 stopwords\n",
      "\n",
      "============================================================\n",
      "LOADING DOCUMENT COLLECTION\n",
      "============================================================\n",
      "✗ Error loading documents: 'utf-8' codec can't decode byte 0xb4 in position 1644: invalid start byte\n",
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "You now have:\n",
      "1. Loaded all news articles\n",
      "2. Cleaned and preprocessed the text\n",
      "3. Created tokens (individual words)\n",
      "4. Removed stopwords and stemmed words\n",
      "\n",
      "Next Phase: We'll implement the Boolean Retrieval Model\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "INFORMATION RETRIEVAL SYSTEM - PHASE 1: DATA PREPROCESSING\n",
    "============================================================\n",
    "This is the foundation of our IR system. We'll load and preprocess the news articles.\n",
    "\n",
    "Author: Your Name\n",
    "Course: Information Retrieval\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "    \n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    \"\"\"\n",
    "    This class handles all text preprocessing tasks.\n",
    "    \n",
    "    Why do we need preprocessing?\n",
    "    - Raw text contains noise (punctuation, special characters)\n",
    "    - Common words like \"the\", \"is\" don't help in retrieval\n",
    "    - Different forms of words (running, runs, ran) should be treated similarly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize stopwords (common words to remove)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize stemmer (reduces words to their root form)\n",
    "        # Example: \"running\" -> \"run\", \"better\" -> \"better\"\n",
    "        self.stemmer = PorterStemmer()\n",
    "        \n",
    "        print(\"✓ Preprocessor initialized\")\n",
    "        print(f\"✓ Loaded {len(self.stop_words)} stopwords\")\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Step 1: Clean the raw text\n",
    "        - Convert to lowercase\n",
    "        - Remove special characters\n",
    "        - Remove extra whitespace\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        \n",
    "        # Remove email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Step 2: Split text into individual words (tokens)\n",
    "        Example: \"I love IR\" -> [\"I\", \"love\", \"IR\"]\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"\n",
    "        Step 3: Remove common words that don't add meaning\n",
    "        Example: [\"the\", \"quick\", \"brown\", \"fox\"] -> [\"quick\", \"brown\", \"fox\"]\n",
    "        \"\"\"\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "    \n",
    "    def stem_tokens(self, tokens):\n",
    "        \"\"\"\n",
    "        Step 4: Reduce words to their root form\n",
    "        Example: [\"running\", \"runs\", \"ran\"] -> [\"run\", \"run\", \"ran\"]\n",
    "        \"\"\"\n",
    "        return [self.stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline\n",
    "        Combines all steps: clean -> tokenize -> remove stopwords -> stem\n",
    "        \"\"\"\n",
    "        # Step 1: Clean\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Step 2: Tokenize\n",
    "        tokens = self.tokenize(cleaned)\n",
    "        \n",
    "        # Step 3: Remove stopwords\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        \n",
    "        # Step 4: Stem\n",
    "        tokens = self.stem_tokens(tokens)\n",
    "        \n",
    "        # Filter out very short tokens (less than 2 characters)\n",
    "        tokens = [token for token in tokens if len(token) > 1]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "class DocumentCollection:\n",
    "    \"\"\"\n",
    "    This class manages the entire document collection.\n",
    "    It loads, preprocesses, and stores all documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        \"\"\"\n",
    "        Initialize and load the document collection\n",
    "        \n",
    "        Args:\n",
    "            csv_path: Path to your CSV file containing news articles\n",
    "        \"\"\"\n",
    "        self.preprocessor = DocumentPreprocessor()\n",
    "        self.documents = []  # Stores original documents\n",
    "        self.processed_docs = []  # Stores preprocessed documents\n",
    "        self.doc_ids = []  # Document identifiers\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING DOCUMENT COLLECTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load the CSV file\n",
    "        self.load_documents(csv_path)\n",
    "        \n",
    "    def load_documents(self, csv_path):\n",
    "        \"\"\"\n",
    "        Load documents from CSV file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(csv_path)\n",
    "            \n",
    "            print(f\"\\n✓ Loaded {len(df)} documents from CSV\")\n",
    "            print(f\"✓ Columns available: {df.columns.tolist()}\")\n",
    "            \n",
    "            # Assume the dataset has 'title' and 'content' columns\n",
    "            # Adjust these column names based on your actual dataset\n",
    "            for idx, row in df.iterrows():\n",
    "                # Combine title and content for better retrieval\n",
    "                # You can modify this based on your dataset structure\n",
    "                doc_text = \"\"\n",
    "                \n",
    "                if 'title' in df.columns:\n",
    "                    doc_text += str(row['title']) + \" \"\n",
    "                if 'content' in df.columns or 'text' in df.columns or 'article' in df.columns:\n",
    "                    content_col = 'content' if 'content' in df.columns else ('text' if 'text' in df.columns else 'article')\n",
    "                    doc_text += str(row[content_col])\n",
    "                \n",
    "                # Store original document\n",
    "                self.documents.append({\n",
    "                    'id': idx,\n",
    "                    'text': doc_text,\n",
    "                    'original': row.to_dict()\n",
    "                })\n",
    "                \n",
    "                self.doc_ids.append(idx)\n",
    "            \n",
    "            print(f\"\\n✓ Stored {len(self.documents)} documents\")\n",
    "            \n",
    "            # Preprocess all documents\n",
    "            self.preprocess_collection()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"✗ Error: File not found at {csv_path}\")\n",
    "            print(\"Please make sure you've downloaded the dataset from Kaggle\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading documents: {e}\")\n",
    "    \n",
    "    def preprocess_collection(self):\n",
    "        \"\"\"\n",
    "        Preprocess all documents in the collection\n",
    "        This is done once and stored for efficiency\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"PREPROCESSING DOCUMENTS\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for idx, doc in enumerate(self.documents):\n",
    "            # Preprocess the document text\n",
    "            processed = self.preprocessor.preprocess(doc['text'])\n",
    "            \n",
    "            self.processed_docs.append({\n",
    "                'id': doc['id'],\n",
    "                'tokens': processed,\n",
    "                'token_count': len(processed)\n",
    "            })\n",
    "            \n",
    "            # Show progress for every 100 documents\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(self.documents)} documents...\")\n",
    "        \n",
    "        print(f\"\\n✓ All {len(self.processed_docs)} documents preprocessed!\")\n",
    "        \n",
    "        # Calculate and display statistics\n",
    "        self.display_statistics()\n",
    "    \n",
    "    def display_statistics(self):\n",
    "        \"\"\"\n",
    "        Display useful statistics about the collection\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COLLECTION STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calculate average document length\n",
    "        avg_length = np.mean([doc['token_count'] for doc in self.processed_docs])\n",
    "        \n",
    "        # Calculate vocabulary size (unique terms)\n",
    "        vocabulary = set()\n",
    "        for doc in self.processed_docs:\n",
    "            vocabulary.update(doc['tokens'])\n",
    "        \n",
    "        print(f\"Total Documents: {len(self.documents)}\")\n",
    "        print(f\"Average Document Length: {avg_length:.2f} tokens\")\n",
    "        print(f\"Vocabulary Size: {len(vocabulary)} unique terms\")\n",
    "        \n",
    "        # Show example of preprocessing\n",
    "        if len(self.documents) > 0:\n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(\"PREPROCESSING EXAMPLE\")\n",
    "            print(\"-\"*60)\n",
    "            example_text = self.documents[0]['text'][:200]\n",
    "            example_tokens = self.processed_docs[0]['tokens'][:20]\n",
    "            \n",
    "            print(f\"Original (first 200 chars):\\n{example_text}...\")\n",
    "            print(f\"\\nAfter preprocessing (first 20 tokens):\\n{example_tokens}\")\n",
    "    \n",
    "    def get_document(self, doc_id):\n",
    "        \"\"\"Get original document by ID\"\"\"\n",
    "        return self.documents[doc_id]\n",
    "    \n",
    "    def get_processed_document(self, doc_id):\n",
    "        \"\"\"Get preprocessed document by ID\"\"\"\n",
    "        return self.processed_docs[doc_id]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    HOW TO USE THIS CODE:\n",
    "    \n",
    "    1. Download the dataset from Kaggle\n",
    "    2. Place the CSV file in the same directory as this script\n",
    "    3. Update the file path below\n",
    "    4. Run this script\n",
    "    \"\"\"\n",
    "    \n",
    "    # Path to your CSV file\n",
    "    CSV_FILE_PATH = r\"C:\\Users\\lenovo\\news_articles.csv\"  \n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMATION RETRIEVAL SYSTEM - PHASE 1\")\n",
    "    print(\"DATA LOADING AND PREPROCESSING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create the document collection\n",
    "    collection = DocumentCollection(CSV_FILE_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1 COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou now have:\")\n",
    "    print(\"1. Loaded all news articles\")\n",
    "    print(\"2. Cleaned and preprocessed the text\")\n",
    "    print(\"3. Created tokens (individual words)\")\n",
    "    print(\"4. Removed stopwords and stemmed words\")\n",
    "    print(\"\\nNext Phase: We'll implement the Boolean Retrieval Model\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51c6cd-18fd-43eb-990e-6da6842b027d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
